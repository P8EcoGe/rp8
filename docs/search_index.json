[["index.html", "R à Paris 8 1 Préface", " R à Paris 8 2021-07-01 1 Préface Ce document est à destination des étudiants de L3, M1 et M2 en économie à l’Université Paris 8. Il a pour objectif de rassembler un ensemble de ressources vous permettant de maîtriser les éléments de base du laguage R utilisé dans vos enseignements de statistique et d’économétrie tout au long de la L3, du M1 et du M2 MBFA. Vos enseignants vous indiqueront les chapitres pertinents pour chaque matière concernée. Ce document n’a pas pour vocation de remplacer le cours théorique de statistique ou d’économétrie que votre enseignant dispense en cours. Il vise juste à rassembler des éléments permettant de les implémenter sous R. Vous pouvez naviguer entre les chapitres à partir de la table des matières à la gauche de la fenêtre de votre navigateur. Sur mobile la navigation peut se faire via les \\(\\equiv\\) en haut de votre écran. "],["R-intro.html", "2 Introduction à R 2.1 Introduction 2.2 Démarrer R et R Studio 2.3 Calculs de base 2.4 Obtenir de l’aide 2.5 Installation de packages 2.6 Code et sortie dans ce document 2.7 Types de données 2.8 Structures de données 2.9 Data Frames 2.10 Les bases de la programmation en R", " 2 Introduction à R 2.1 Introduction R est à la fois un langage de programmation et un logiciel de calcul statistique. Il est gratuit et libre (open-source). Pour commencer, vous allez devoir installer deux logiciels : R, le language de programmation lui même. Sélectionnez votre système d’exploitation et choisissez la version la plus récente de R RStudio, une excellente interface de programmation pour R. Attention, il faut que vous ayez installé R pour pouvoir utiliser RStudio. RStudio est juste une interface permettant d’utiliser R. Les éditeurs du logiciel RStudio ont par ailleurs crée la plate-forme en ligne RStudio Cloud rstudio.cloud.. Cette dernière permet d’utiliser R en ligne depuis son navigateur, de faire des tutoriels d’introduction (voire plus avancés), etc. Afin de vous familiariser au plus vite avec le R et R Studio (pour ceux qui ne connaissent pas déjà R), il faut au plus vite que Vous vous rendiez sur rstudio.cloud. et que vous vous y créiez un compte (Sign Up). La plateforme est en anglais mais ça devrait être compréhensible Une fois votre compté crée, allez sur la page des Primers (tutoriels d’introduction) : https://rstudio.cloud/learn/primers, et effectuez au minimum les tutoriels “The Basics” et “Work with Data”. Idéalement effectuez aussi “Visualize Data” et “Tidy Your Data”. Les plus courageux/motivés pourront également effectuer “Iterate” et “Write Functions”. R Studio Cloud vous permet également d’utiliser R en ligne depuis un navigateur sans avoir à l’installer sur votre machine. C’est extrêmement pratique, mais la version gratuite ne permet que 15 heures d’utilisation par mois pour chaque compte. Son utilisation sera donc réservée aux éventuels cours dans une salle informatique et pour ceux d’entre vous qui sont dans l’impossibilité d’installer R sur leur ordinateur. 2.2 Démarrer R et R Studio Il est fondamental de comprendre la différence entre R, le langage de programmation, et RStudio, l’interface pour R qui vous permet de travailler efficacement (et plus facilement) avec R. La meilleure façon de comprendre l’utilité de RStudio est de commencer par R sans RStudio. Pour ce faire, double-clickez sur le programme R que vous devriez avoir installé sur votre ordinateur en suivant les instructions ci-dessus (sous windows ou Mac), ou lancez R depuis votre terminal (sous Linux ou Mac) en tapant R dans un terminal, voir 2.2. Vous venez d’ouvrir la console R qui vous permet de commencer à taper du code juste après le symbole &gt;, appelé prompt. Essayez de taper 2 + 2 or print(\"Votre Nom\") puis appuyez sur la touche Entrée. Et voilà, votre première commande R Figure 2.1: Le logo de R Figure 2.2: Un terminal R sous Linux Taper une commande après l’autre dans la console n’est pas très pratique au fur et à mesure que notre analyse devient plus complexe. L’idéal serait de rassembler toutes les instructions de commande dans un fichier et de les exécuter l’une après l’autre, automatiquement. Nous pouvons le faire en écrivant des fichiers dits script , c’est-à-dire de simples fichiers texte avec l’extension .R ou .r qui peuvent être insérés (ou sourcés) dans une session R. RStudio rend ce processus très simple. Ouvrez RStudio en cliquant sur l’application RStudio sur votre ordinateur, et remarquez à quel point l’environnement entier est différent de la console R de base - en fait, cette console R fonctionne dans votre panneau inférieur gauche. Le panneau supérieur gauche est un espace où vous pouvez écrire des scripts, c’est-à-dire de nombreuses lignes de codes que vous pouvez exécuter quand vous le souhaitez. Pour exécuter une seule ligne de code, il suffit de la mettre en surbrillance et d’appuyer sur “Ctrl” + “Entrée”. RStudio dispose d’un grand nombre de raccourcis clavier utiles. Une liste de ceux-ci peut être trouvée en utilisant un raccourci clavier – le raccourci clavier qui permet de tous les utiliser : Sous Windows ou linux: Alt + Shift + K Sous Mac: Option + Shift + K L’équipe de R Studio a développé [un certain nombre de “cheatsheets”] (https://www.rstudio.com/resources/cheatsheets/) pour travailler à la fois avec R et R Studio. Cette fiche particulière pour la base R] (http://www.rstudio.com/wp-content/uploads/2016/05/base-r.pdf) résume un grand nombre des concepts de ce document. 2.2.1 Un premier glossaire R : un langage de programmation statistique R Studio : un environnement de développement intégré (IDE) pour travailler avec R. commande : saisie de l’utilisateur (texte ou chiffres) que R comprend. script : une liste de commandes rassemblées dans un fichier texte, chacune étant séparée par une nouvelle ligne, à exécuter l’une après l’autre. 2.3 Calculs de base Pour commencer, nous utiliserons le R comme une simple calculatrice. Exécutez le code suivant soit directement depuis votre console RStudio, soit dans RStudio en l’écrivant dans un script et en l’exécutant en utilisant “Ctrl” + “Entrée”. Addition, Sousraction, Multiplication et Division Math code R Résultat \\(3 + 2\\) 3 + 2 5 \\(3 - 2\\) 3 - 2 1 \\(3 \\cdot2\\) 3 * 2 6 \\(3 / 2\\) 3 / 2 1.5 Exposants Math code R Résultat \\(3^2\\) 3 ^ 2 9 \\(2^{(-3)}\\) 2 ^ (-3) 0.125 \\(100^{1/2}\\) 100 ^ (1 / 2) 10 \\(\\sqrt{100}\\) sqrt(100) 10 Constantes mathématiques Math code R Résultat \\(\\pi\\) pi 3.1415927 \\(e\\) exp(1) 2.7182818 Logarithmes Notez que nous utiliserons \\(\\ln\\) et \\(\\log\\) de manière interchangeable pour désigner le logarithme naturel. Il n’y a pas de ln() dans R, mais on utilise log() pour désigner le logarithme naturel. Math code R Résultat \\(\\log(e)\\) log(exp(1)) 1 \\(\\log_{10}(1000)\\) log10(1000) 3 \\(\\log_{2}(8)\\) log2(8) 3 \\(\\log_{4}(16)\\) log(16, base = 4) 2 Trigonometrie Math code R Résultat \\(\\sin(\\pi / 2)\\) sin(pi / 2) 1 \\(\\cos(0)\\) cos(0) 1 2.4 Obtenir de l’aide En utilisant R comme calculatrice, nous avons vu un certain nombre de fonctions : sqrt(), exp(), log() et sin(). Pour obtenir de la documentation sur une fonction dans R, il suffit de mettre un point d’interrogation devant le nom de la fonction, ou d’appeler la fonction help(function) et RStudio affichera la documentation, par exemple : ?log ?sin ?paste ?lm help(lm) # help() est équivalent help(ggplot,package=&quot;ggplot2&quot;) # affiche l&#39;aide d&#39;un package donné Souvent, l’une des choses les plus difficiles à faire quand on apprend R est de demander de l’aide. D’abord, vous devez décider de demander de l’aide, puis vous devez savoir comment demander de l’aide. Votre toute première ligne de défense devrait être de rechercher sur Google votre message d’erreur ou une brève description de votre problème. (La capacité à résoudre des problèmes à l’aide de cette méthode devient rapidement une compétence extrêmement précieuse). Si cela échoue, et cela finira par arriver, vous devriez demander de l’aide. Il y a un certain nombre de choses que vous devez inclure lorsque vous contactez un enseignant ou que vous postez sur un site d’aide tel que [Stack Overflow] (https://stackoverflow.com). Décrivez ce que vous attendez du code. Indiquez l’objectif final que vous essayez d’atteindre. (Parfois, ce que vous attendez du code n’est pas ce que vous voulez réellement faire). Fournissez le texte intégral de toutes les erreurs que vous avez reçues. Fournissez suffisamment de code pour recréer l’erreur. Parfois, il est également utile d’inclure une capture d’écran de toute votre fenêtre RStudio lorsque l’erreur se produit. Si vous suivez ces étapes, votre problème sera résolu beaucoup plus rapidement, et vous en apprendrez peut-être plus au cours du processus. Ne vous découragez pas en rencontrant des erreurs et des difficultés lors de l’apprentissage de R\". (Ou toute autre compétence technique.) Cela fait tout simplement partie du processus d’apprentissage. 2.5 Installation de packages R est livré avec un certain nombre de fonctions et d’ensembles de données intégrés, mais l’une des principales forces de R en tant que projet open-source est son système de paquets (packages). Les paquets ajoutent des fonctions et des données supplémentaires. Souvent, si vous voulez faire quelque chose dans R, et que ce n’est pas disponible par défaut, il y a de fortes chances qu’il existe un paquet qui répondra à vos besoins. Pour installer un paquet, utilisez la fonction install.packages(). Imaginez que vous achetez un livre de recettes au magasin, que vous le rapportez à la maison et que vous le mettez sur votre étagère (c’est-à-dire dans votre bibliothèque) : install.packages(&quot;ggplot2&quot;) Une fois qu’un paquet est installé, il doit être chargé dans votre session R avant d’être utilisé. Cela revient à retirer le livre de l’étagère et à l’ouvrir pour le lire. library(ggplot2) Une fois que vous fermez R, tous les paquets sont fermés et remis sur l’étagère imaginaire. La prochaine fois que vous ouvrirez R, vous n’aurez pas à réinstaller le paquet, mais vous devrez charger tous les paquets que vous avez l’intention d’utiliser en invoquant library(). 2.6 Code et sortie dans ce document Pour distinguer visuellement le code R de sa sortie, toutes le lignes de sortie commencent par ##. Un bout de code typique avec sa sortie va ressembler à ça : 1 + 3 ## [1] 4 # tout ce qui se situe après un # est un commentaire, R ne l&#39;exécute pas où vous voyez sur la première ligne le code R, et sur la deuxième ligne la sortie. Comme mentionné, cette ligne commence par ## pour dire c’est une sortie, suivi par [1] (indiquant qu’il s’agit d’un vecteur de longueur un - plus d’informations ci-dessous !), suivi par le résultat réel - 1 + 3 = 4 ! Notez que vous pouvez simplement copier et coller tout le code que vous voyez dans votre console R. En fait, vous êtes fortement encouragé à le faire et à essayer tout le code que vous voyez dans ce livre. 2.7 Types de données R a un certain nombre de types de données de base. Il est utile de savoir quels types de données sont à votre disposition : Numérique Aussi connu sous le nom de Double. Le type par défaut lorsqu’il s’agit de chiffres. Exemples : 1, 1.0, 42.5 Entier Exemples : 1L, 2L, 42L. Complexe Exemple : 4 + 2i. Logique Deux valeurs possibles : TRUE et FALSE Vous pouvez également utiliser les lettres T et F, mais cela n’est pas recommandé. NA est également considéré comme logique. Caractère Exemples : \"a\", \"Statistiques\", \"1 plus 2\". Catégorique ou factor. Un mélange d’entier et de caractère. Une variable factor attribue un label à une valeur numérique. Par exemple, factor(x=c(0,1),labels=c(\"homme\", \"femme\")) assigne la chaîne homme aux valeurs numériques 0, et la chaîne femme à la valeur 1. 2.8 Structures de données Dimension Homogène Hétérogène 1 Vector List 2 Matrix Data Frame 3+ Array nested Lists 2.8.1 Vecteurs De nombreuses opérations en R font un usage intensif de vecteurs. Un vecteur est un conteneur pour les objets de type identique (voir 2.7 ci-dessus). Les vecteurs dans R sont indexés à partir de 1. C’est ce que le [1] dans la sortie indique, que le premier élément de la ligne affichée est le premier élément du vecteur. Les plus grands vecteurs afficheront des lignes supplémentaires avec quelque chose comme [7] où 7 est l’indice du premier élément de cette ligne. La façon la plus courante de créer un vecteur en R est sans doute d’utiliser la fonction c(), qui est l’abréviation de “combine”. Comme son nom l’indique, elle combine une liste d’éléments séparés par des virgules. c(1, 3, 5, 7, 8, 9) ## [1] 1 3 5 7 8 9 Ici, R produit simplement ce vecteur. Si nous voulons stocker ce vecteur dans une variable, nous pouvons le faire avec l’opérateur assignation &lt;-. Dans ce cas, la variable x contient maintenant le vecteur que nous venons de créer, et nous pouvons accéder au vecteur en tapant x. x &lt;- c(1, 3, 5, 7, 8, 9) x ## [1] 1 3 5 7 8 9 Parce que les vecteurs doivent contenir des éléments qui sont tous du même type, R va automatiquement coercer (c’est-à-dire convertir) vers un seul type lorsqu’on essaie de créer un vecteur qui combine plusieurs types. c(42, &quot;Statistiques&quot;, TRUE) ## [1] &quot;42&quot; &quot;Statistiques&quot; &quot;TRUE&quot; c(42, TRUE) ## [1] 42 1 Il est fréquent que vous souhaitiez créer un vecteur basé sur une séquence de nombres. La façon la plus rapide et la plus simple de le faire est d’utiliser l’opérateur :, qui crée une séquence d’entiers entre deux entiers spécifiés. (y &lt;- 1:100) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 Ici, nous voyons R marquer les lignes après la première, car il s’agit d’un grand vecteur. Nous voyons également qu’en mettant des parenthèses autour de l’affectation, R stocke le vecteur dans une variable appelée y et sort automatiquement y vers la console. Notez que les scalaires n’existent pas dans R. Ce sont simplement des vecteurs de longueur 1. 2 ## [1] 2 Si nous voulons créer une séquence qui ne soit pas limitée aux nombres entiers et qui augmente de 1 à la fois, nous pouvons utiliser la fonction seq(). seq(from = 1.5, to = 4.2, by = 0.1) ## [1] 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 ## [20] 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 Nous discuterons des fonctions en détail plus tard, mais notez ici que les étiquettes d’entrée “from”, “to” et “by” sont facultatives. seq(1.5, 4.2, 0.1) ## [1] 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 ## [20] 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 Une autre opération courante pour créer un vecteur est rep(), qui peut répéter une seule valeur un certain nombre de fois. rep(&quot;A&quot;, times = 10) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; La fonction rep() peut être utilisée pour répéter un vecteur un certain nombre de fois. rep(x, times = 3) ## [1] 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 Nous avons maintenant vu quatre façons différentes de créer des vecteurs : c() : seq() rep() Jusqu’à présent, nous les avons surtout utilisés de manière isolée, mais ils sont souvent utilisés ensemble. c(x, rep(seq(1, 9, 2), 3), c(1, 2, 3), 42, 2:4) ## [1] 1 3 5 7 8 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 2 3 42 ## [26] 2 3 4 La longueur d’un vecteur peut être obtenue avec la fonction length(). length(x) ## [1] 6 length(y) ## [1] 100 2.8.1.1 Exercice 1 Créer un vecteur de cinq uns, c’est-à-dire [1,1,1,1,1,1]. Remarquez que l’opérateur deux-points a:b est juste l’abréviation de construire une séquence de a à b. Créez un vecteur dont le compte à rebours est de 10 à 0, c’est-à-dire qu’il ressemble à [10,9,8,7,6,5,4,3,2,1,0] ! la fonction rep prend des arguments supplémentaires times (comme ci-dessus), et each, qui vous indique combien de fois chaque élément doit être répété (par opposition au vecteur d’entrée entier). Utilisez rep pour créer un vecteur qui ressemble à ceci : [1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3] 2.8.1.2 Extraire des sous-vecteurs Pour extraire un sous-vecteur (subsetting), c’est-à-dire pour n’en choisir que certains éléments, nous utilisons des crochets, “[]”. Nous voyons ici que x[1] renvoie le premier élément, et x[3] renvoie le troisième élément : x ## [1] 1 3 5 7 8 9 x[1] ## [1] 1 x[3] ## [1] 5 Nous pouvons également exclure certains indices, dans ce cas le deuxième élément. x[-2] ## [1] 1 5 7 8 9 Enfin, nous voyons que nous pouvons faire des sous-ensembles sur la base d’un vecteur d’indices. x[1:3] ## [1] 1 3 5 x[c(1,3,4)] ## [1] 1 5 7 Tous les éléments ci-dessus sont des sous-ensembles d’un vecteur utilisant un vecteur d’indices. (Rappelez-vous qu’un seul nombre reste un vecteur) Nous pourrions plutôt utiliser un vecteur de valeurs logiques. z = c(TRUE, TRUE, FALSE, TRUE, TRUE, FALSE) z ## [1] TRUE TRUE FALSE TRUE TRUE FALSE x[z] ## [1] 1 3 7 8 R est capable d’effectuer de nombreuses opérations sur les vecteurs et les scalaires : x = 1:10 # un vecteur x + 1 # ajouter un scalaire ## [1] 2 3 4 5 6 7 8 9 10 11 2 * x # multiplier tous les éléments par 2 ## [1] 2 4 6 8 10 12 14 16 18 20 2 ^ x # calculer 2 exposant x ## [1] 2 4 8 16 32 64 128 256 512 1024 sqrt(x) # calcule la racine carrée de tous les éléments de x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 log(x) # prend le log naturel de tous les éléments en x ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101 ## [8] 2.0794415 2.1972246 2.3025851 x + 2*x # ajouter le vecteur x au vecteur 2x ## [1] 3 6 9 12 15 18 21 24 27 30 Nous voyons que lorsqu’une fonction comme log() est utilisée sur un vecteur x, un vecteur est renvoyé qui a appliqué la fonction à chaque élément du vecteur x. 2.8.2 Opérateurs logiques Opérateur Résumé Exemple Résultat x &lt; y x plus petit que y 3 &lt; 42 TRUE x &gt; y x plus grand que y 3 &gt; 42 FALSE x &lt;= y x plus petit ou égal à y 3 &lt;= 42 TRUE x &gt;= y x plus grand ou égal à y 3 &gt;= 42 FALSE x == y x égal à y 3 == 42 FALSE x != y x pas égal à y 3 != 42 TRUE !x pas x !(3 &gt; 42) TRUE x | y x ou y (3 &gt; 42) | TRUE TRUE x &amp; y x et y (3 &lt; 4) &amp; ( 42 &gt; 13) TRUE Dans R, les opérateurs logiques marchent également sur les vecteurs : x = c(1, 3, 5, 7, 8, 9) x &gt; 3 ## [1] FALSE FALSE TRUE TRUE TRUE TRUE x &lt; 3 ## [1] TRUE FALSE FALSE FALSE FALSE FALSE x == 3 ## [1] FALSE TRUE FALSE FALSE FALSE FALSE x != 3 ## [1] TRUE FALSE TRUE TRUE TRUE TRUE x == 3 &amp; x != 3 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE x == 3 | x != 3 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE C’est très utile pour sélectionner une sous-partie d’un vecteur. x[x &gt; 3] ## [1] 5 7 8 9 x[x != 3] ## [1] 1 5 7 8 9 sum(x &gt; 3) ## [1] 4 as.numeric(x &gt; 3) ## [1] 0 0 1 1 1 1 Nous avons vu ici que l’utilisation de la fonction sum() sur un tableau de valeurs logiques Vrai et Faux qui est le résultat de x &gt; 3 donne un résultat numérique : vous venez de compter combien d’éléments de x, la condition &gt; 3 est Vrai. Lors de l’appel à sum(), R est d’abord automatiquement forcé de passer de la logique au numérique où Vrai est 1 et Faux est 0. Cette contrainte de logique à numérique se produit pour la plupart des opérations mathématiques. # which(condition sur x) renvoie true/false # chaque indice de x où la condition est vraie which(x &gt; 3) ## [1] 3 4 5 6 x [which(x &gt; 3)] ## [1] 5 7 8 9 max(x) ## [1] 9 which(x == max(x)) ## [1] 6 which.max(x) ## [1] 6 2.8.2.1 Exercice 2 Créer un vecteur rempli de 10 nombres tirés de la distribution uniforme (indice : utiliser la fonction runif) et les stocker dans “x”. En utilisant un sous-ensemble logique comme ci-dessus, obtenez tous les éléments de x qui sont plus grands que 0,5, et stockez-les dans y. en utilisant la fonction which, stockez les indices de tous les éléments de x qui sont plus grands que 0,5 dans iy. Vérifiez que y et x[iy] sont identiques. 2.8.3 Matrices R peut également être utilisé pour les calculs de matrice. Les matrices ont des lignes et des colonnes contenant un seul type de données. Dans une matrice, l’ordre des lignes et des colonnes est important. (Ce n’est pas le cas des data frames, que nous verrons plus tard). Les matrices peuvent être créées à l’aide de la fonction matrix. x = 1:9 x ## [1] 1 2 3 4 5 6 7 8 9 X = matrix(x, nrow = 3, ncol = 3) X ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Remarquez ici que R est sensible à la casse (x contre X). Par défaut, la fonction matrice remplit vos données dans la matrice colonne par colonne. Mais on peut aussi dire à R de remplir des lignes à la place : Y = matrix(x, nrow = 3, ncol = 3, byrow = TRUE) Y ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 Nous pouvons également créer une matrice d’une dimension spécifique où chaque élément est identique, dans ce cas “0”. Z = matrix(0, 2, 4) Z ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 Tout comme les vecteurs, on peut extraire une sous-matrices des crochets, “[]”. Cependant, comme les matrices sont bidimensionnelles, nous devons spécifier à la fois une ligne et une colonne lors du sous-ensemble. X ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 X[1, 2] ## [1] 4 Ici, nous avons accédé à l’élément de la première ligne et de la deuxième colonne. Nous avons également pu extraire une ligne ou une colonne entière. X[1, ] ## [1] 1 4 7 X[, 2] ## [1] 4 5 6 Nous pouvons également utiliser des vecteurs pour extraire plus d’une ligne ou d’une colonne à la fois. Dans ce cas, on extrait la première et la troisième colonne de la deuxième ligne : X [2, c(1, 3)] ## [1] 2 8 Les matrices peuvent également être créées en combinant des vecteurs sous forme de colonnes, en utilisant cbind, ou en combinant des vecteurs sous forme de lignes, en utilisant rbind. x = 1:9 rev(x) ## [1] 9 8 7 6 5 4 3 2 1 rep(1, 9) ## [1] 1 1 1 1 1 1 1 1 1 rbind(x, rev(x), rep(1, 9)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## x 1 2 3 4 5 6 7 8 9 ## 9 8 7 6 5 4 3 2 1 ## 1 1 1 1 1 1 1 1 1 cbind(col_1 = x, col_2 = rev(x), col_3 = rep(1, 9)) ## col_1 col_2 col_3 ## [1,] 1 9 1 ## [2,] 2 8 1 ## [3,] 3 7 1 ## [4,] 4 6 1 ## [5,] 5 5 1 ## [6,] 6 4 1 ## [7,] 7 3 1 ## [8,] 8 2 1 ## [9,] 9 1 1 Lorsque vous utilisez rbind et cbind, vous pouvez spécifier des noms d’“arguments” qui seront utilisés comme noms de colonnes. R peut alors être utilisé pour effectuer des calculs matriciels x = 1:9 y = 9:1 X = matrix(x, 3, 3) Y = matrix(y, 3, 3) X ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Y ## [,1] [,2] [,3] ## [1,] 9 6 3 ## [2,] 8 5 2 ## [3,] 7 4 1 X + Y ## [,1] [,2] [,3] ## [1,] 10 10 10 ## [2,] 10 10 10 ## [3,] 10 10 10 X - Y ## [,1] [,2] [,3] ## [1,] -8 -2 4 ## [2,] -6 0 6 ## [3,] -4 2 8 X * Y ## [,1] [,2] [,3] ## [1,] 9 24 21 ## [2,] 16 25 16 ## [3,] 21 24 9 X / Y ## [,1] [,2] [,3] ## [1,] 0.1111111 0.6666667 2.333333 ## [2,] 0.2500000 1.0000000 4.000000 ## [3,] 0.4285714 1.5000000 9.000000 Notez que X * Y n’est pas une multiplication matricielle. Il s’agit d’une multiplication élément par élément. (Pareil pour X / Y). La multiplication matricielle utilise l’opérateur %*%. Les autres fonctions matricielles comprennent t() qui donne la transposition d’une matrice et solve() qui renvoie l’inverse d’une matrice carrée si elle est inversible. X %*% Y ## [,1] [,2] [,3] ## [1,] 90 54 18 ## [2,] 114 69 24 ## [3,] 138 84 30 t(X) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 2.8.3.1 Exercice 3 Créer un vecteur contenant 1,2,3,4,5 appelé v. Créez une matrice (2,5) m contenant les données 1,2,3,4,5,6,7,8,9,10. La première ligne doit être “1,2,3,4,5”. Effectuez la multiplication de la matrice m par v. Utilisez la commande %*%. Quelle est la dimension de la sortie ? Pourquoi la commandev %*% m ne fonctionne-t-elle pas ? 2.9 Data Frames Nous avons déjà vu des vecteurs et des matrices pour le stockage des données lorsque nous avons présenté R. Nous allons maintenant présenter les data frame (bases de données) qui seront la façon la plus courante de stocker et d’interagir avec les données. example_data = data.frame(x = c(1, 3, 5, 7, 9, 1, 3, 5, 7, 9), y = c(rep(&quot;Hello&quot;, 9), &quot;Goodbye&quot;), z = rep(c(TRUE, FALSE), 5)) Contrairement à une matrice, qui peut être considérée comme un vecteur réorganisé en lignes et en colonnes, une data frame n’est pas tenue d’avoir le même type de données pour chaque élément. Une data frame est une liste de vecteurs, et chaque vecteur a un nom. Ainsi, chaque vecteur doit contenir le même type de données, mais les différents vecteurs peuvent stocker des types de données différents. Notez, cependant, que tous les vecteurs doivent avoir la même longueur. Une data.frame est similaire à un tableur habituel. Il y a des lignes, et des colonnes. Une ligne est généralement considérée comme une observation, et chaque colonne est une certaine variable, ou caractéristique de cette observation. Regardons la data frame que nous venons de créer example_data ## x y z ## 1 1 Hello TRUE ## 2 3 Hello FALSE ## 3 5 Hello TRUE ## 4 7 Hello FALSE ## 5 9 Hello TRUE ## 6 1 Hello FALSE ## 7 3 Hello TRUE ## 8 5 Hello FALSE ## 9 7 Hello TRUE ## 10 9 Goodbye FALSE Là encore, nous accédons à une colonne donnée avec l’opérateur $ : example_data$x ## [1] 1 3 5 7 9 1 3 5 7 9 all.equal(length(example_data$x), length(example_data$y), length(example_data$z)) ## [1] TRUE str(example_data) ## &#39;data.frame&#39;: 10 obs. of 3 variables: ## $ x: num 1 3 5 7 9 1 3 5 7 9 ## $ y: chr &quot;Hello&quot; &quot;Hello&quot; &quot;Hello&quot; &quot;Hello&quot; ... ## $ z: logi TRUE FALSE TRUE FALSE TRUE FALSE ... nrow(example_data) ## [1] 10 ncol(example_data) ## [1] 3 dim(example_data) ## [1] 10 3 names(example_data) ## [1] &quot;x&quot; &quot;y&quot; &quot;z&quot; 2.9.1 Travailler sur des data.frames La fonction data.frame() ci-dessus est une façon de créer une base de données. Nous pouvons également importer des données de différents types de fichiers dans R, ainsi qu’utiliser des données stockées dans des paquets. Pour recharger ces données dans R, nous utiliserons la fonction read.csv : example_data_from_disk = read.csv(&quot;data/example-data.csv&quot;) example_data_from_disk ## x y z ## 1 1 Hello TRUE ## 2 3 Hello FALSE ## 3 5 Hello TRUE ## 4 7 Hello FALSE ## 5 9 Hello TRUE ## 6 1 Hello FALSE ## 7 3 Hello TRUE ## 8 5 Hello FALSE ## 9 7 Hello TRUE ## 10 9 Goodbye FALSE Lorsque nous utilisons des données, il y a généralement trois choses que nous aimerions faire : Examiner les données brutes. Comprendre les données. (D’où viennent-elles ? Quelles sont les variables ? etc.) Visualiser les données. Pour regarder les données dans un data.frame, nous avons deux commandes utiles : head() et str(). # Nous travaillons avec l&#39;ensemble de données mtcars intégré dans R : mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Vous pouvez voir que cela imprime l’intégralité du data.frame à l’écran. La fonction head() affichera les premières n observations de la trame de données. head(mtcars,n=2) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21 6 160 110 3.9 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21 6 160 110 3.9 2.875 17.02 0 1 4 4 head(mtcars) # défaut ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 La fonction str() affichera la “structure” du cadre de données. Elle affichera le nombre d’ observations et de variables, énumérera les variables, donnera le type de chaque variable, et montrera certains éléments de chaque variable. Ces informations peuvent également être trouvées dans la fenêtre “Environnement” de RStudio. str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... Dans cet base de données, une observation porte sur un modèle particulier de voiture, et les variables décrivent les attributs de la voiture, par exemple son rendement énergétique ou son poids. Pour en savoir plus sur l’ensemble de données, nous utilisons l’opérateur ? pour extraire la documentation relative aux données. ?mtcars R dispose d’un certain nombre de fonctions permettant de travailler et d’extraire rapidement des informations de base à partir de cadres de données. Pour obtenir rapidement un vecteur des noms de variables, nous utilisons la fonction names(). names(mtcars) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [11] &quot;carb&quot; Pour accéder à l’une des variables en tant que vecteur, nous utilisons l’opérateur $. mtcars$mpg ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 ## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 ## [31] 15.0 21.4 mtcars$wt ## [1] 2.620 2.875 2.320 3.215 3.440 3.460 3.570 3.190 3.150 3.440 3.440 4.070 ## [13] 3.730 3.780 5.250 5.424 5.345 2.200 1.615 1.835 2.465 3.520 3.435 3.840 ## [25] 3.845 1.935 2.140 1.513 3.170 2.770 3.570 2.780 Nous pouvons utiliser les fonctions dim(), nrow() et ncol() pour obtenir des informations sur la dimension de la base de données. dim(mtcars) ## [1] 32 11 nrow(mtcars) ## [1] 32 ncol(mtcars) ## [1] 11 Ici nrow() est aussi le nombre d’observations, qui dans la plupart des cas est la taille de l’échantillon. La sélection de sous-partie de la base de données peut fonctionner comme pour les matrices en utilisant des crochets, [ , ]. Ici, nous trouvons des véhicules dont le mpg est supérieur à 25 miles par gallon et nous n’affichons que les colonnes cyl, disp et wt. # mpg [condition ligne, condition colonne] mtcars [mtcars$mpg &gt; 20, c(&quot;cyl&quot;, &quot;disp&quot;, &quot;wt&quot;)] ## cyl disp wt ## Mazda RX4 6 160.0 2.620 ## Mazda RX4 Wag 6 160.0 2.875 ## Datsun 710 4 108.0 2.320 ## Hornet 4 Drive 6 258.0 3.215 ## Merc 240D 4 146.7 3.190 ## Merc 230 4 140.8 3.150 ## Fiat 128 4 78.7 2.200 ## Honda Civic 4 75.7 1.615 ## Toyota Corolla 4 71.1 1.835 ## Toyota Corona 4 120.1 2.465 ## Fiat X1-9 4 79.0 1.935 ## Porsche 914-2 4 120.3 2.140 ## Lotus Europa 4 95.1 1.513 ## Volvo 142E 4 121.0 2.780 Une alternative serait d’utiliser la fonction subset(), qui a une syntaxe beaucoup plus lisible. subset(mtcars, subset = mpg &gt; 25, select = c(&quot;cyl&quot;, &quot;disp&quot;, &quot;wt&quot;)) 2.9.1.1 Exercice 5 Combien d’observations y a-t-il dans mtcars ? Combien de variables ? Quelle est la valeur moyenne de mpg ? Quelle est la valeur moyenne de mpg pour les voitures de plus de 4 cylindres, c’est-à-dire avec cyl&gt;4 ? 2.10 Les bases de la programmation en R Dans cette section, nous illustrons quelques concepts généraux liés à la programmation en R. 2.10.1 Variables Nous avons déjà rencontré le terme de “variable”\" à plusieurs reprises, mais principalement dans le contexte d’une colonne d’une data.frame. En programmation, une variable désigne un “objet”. Une autre façon de le dire est qu’une variable est un nom ou une étiquette pour quelque chose : x &lt;- 1 y &lt;- &quot;roses&quot; z &lt;- function(x){sqrt(x)} Ici, x fait référence à la valeur 1, y contient la chaîne de caractères “roses”, et z est le nom d’une fonction qui calcule \\(\\sqrt{x}\\). Remarquez que l’argument x de la fonction est différent du x que nous venons de définir. Il est local à la fonction et n’affecte pas le x défini à lextérieur de la fonction : x ## [1] 1 z(9) ## [1] 3 2.10.2 Contrôle du flux, conditions “si, alors” Le contrôle de flux d’execution concerne les moyens par lesquels vous pouvez adapter votre code à différentes circonstances. Si une “condition” est TRUE, votre programme fera une chose, et sinon une autre. C’est ce que l’on appelle une déclaration “if/else”. Dans R, la syntaxe if/else est : if (condition = TRUE) { un peu de code R } else { un autre code R } Par exemple, x &lt;- 1 y &lt;- 3 if (x &gt; y) { # test si x &gt; y # si VRAI z &lt;- x * y print(&quot;x est plus grand que y&quot;) } else { # si FAUX z &lt;- x + 5 * y print(&quot;x est inférieur ou égal à y&quot;) } ## [1] &quot;x est inférieur ou égal à y&quot; z ## [1] 16 2.10.3 Boucles Les boucles sont un élément de programmation très important. Comme son nom l’indique, dans une boucle, la programmation passe en boucle de façon répétée sur un ensemble d’instructions, jusqu’à ce qu’une condition lui dise de s’arrêter. Par ailleurs, le programme peut savoir combien d’étapes il a déjà effectuées - ce qui peut être important à savoir pour de nombreux algorithmes. La syntaxe d’une boucle for (il en existe d’autres) est for (ix in 1:10){ # ne doit pas nécessairement être 1:10 ! # corps de la boucle : est exécuté à chaque fois # la valeur de ix change à chaque itération } Par exemple, considérons cette simple boucle for, qui va simplement imprimer la valeur de l’itérateur (appelé i dans notre cas) à l’écran : for (i in 1:5){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Remarquez qu’au lieu de “1:5”, nous pourrions avoir n’importe quelle sorte de collection d’éléments sur lesquels itérer : for (i in c(&quot;mangues&quot;, &quot;bananes&quot;, &quot;pommes&quot;)){ print(paste(&quot;J&#39;aime les&quot;,i)) # la fonction de collage &quot;paste&quot; colle les chaînes de caractères } ## [1] &quot;J&#39;aime les mangues&quot; ## [1] &quot;J&#39;aime les bananes&quot; ## [1] &quot;J&#39;aime les pommes&quot; Nous pouvons \"aussi souvent également utiliser des boucles emboîtées, qui sont exactement ce que leur nom suggère : for (i in 2:3){ # premier niveau : pour chaque i for (j in c(&quot;mangues&quot;, &quot;bananes&quot;, &quot;pommes&quot;)){ # deuxième niveau : pour chaque j print(paste(&quot;Puis-je avoir&quot;,i,j, &quot;s&#39;il vous plaît ?&quot;)) } } ## [1] &quot;Puis-je avoir 2 mangues s&#39;il vous plaît ?&quot; ## [1] &quot;Puis-je avoir 2 bananes s&#39;il vous plaît ?&quot; ## [1] &quot;Puis-je avoir 2 pommes s&#39;il vous plaît ?&quot; ## [1] &quot;Puis-je avoir 3 mangues s&#39;il vous plaît ?&quot; ## [1] &quot;Puis-je avoir 3 bananes s&#39;il vous plaît ?&quot; ## [1] &quot;Puis-je avoir 3 pommes s&#39;il vous plaît ?&quot; Ce qu’il est important de remarquer ici, c’est que vous pouvez faire des calculs avec les itérateurs tout en restant dans une boucle. for (i in 1:4) { for (j in 1:4) { ifoisj &lt;- i*j print(paste(i,&quot;fois&quot;,j,&quot;=&quot;,ifoisj)) } } ## [1] &quot;1 fois 1 = 1&quot; ## [1] &quot;1 fois 2 = 2&quot; ## [1] &quot;1 fois 3 = 3&quot; ## [1] &quot;1 fois 4 = 4&quot; ## [1] &quot;2 fois 1 = 2&quot; ## [1] &quot;2 fois 2 = 4&quot; ## [1] &quot;2 fois 3 = 6&quot; ## [1] &quot;2 fois 4 = 8&quot; ## [1] &quot;3 fois 1 = 3&quot; ## [1] &quot;3 fois 2 = 6&quot; ## [1] &quot;3 fois 3 = 9&quot; ## [1] &quot;3 fois 4 = 12&quot; ## [1] &quot;4 fois 1 = 4&quot; ## [1] &quot;4 fois 2 = 8&quot; ## [1] &quot;4 fois 3 = 12&quot; ## [1] &quot;4 fois 4 = 16&quot; Les Primers de rstudio.cloud contiennent un tutoriel sur les boucles qui va au delà de cette brève introduction, et introduite de nouveaux outils 2.10.4 Fonctions Jusqu’à présent, nous avons utilisé des fonctions, mais nous n’avons pas vraiment discuté de certains de leurs détails. Une fonction est un ensemble d’instructions que R exécute pour nous, un peu comme celles qu’on écrit dans un script. L’avantage est que les fonctions sont beaucoup plus flexibles que les scripts, car elles peuvent dépendre d’arguments en entrée, qui modifient le comportement de la fonction. Voici comment définir une fonction : nom_de_la_fonction &lt;- function(arg1,arg2=valeur_par_défaut){ # corps de la fonction # faire des choses avec arg1 et arg2 # vous pouvez avoir un nombre illimité d&#39;arguments, avec ou sans défaut # Toute commande &quot;R&quot; valide peut être incluse ici # la dernière ligne est renvoyée comme résultat de la fonction } Et voici un exemple trivial de définition d’une fonction : bonjour &lt;- function(votre_nom = &quot;Lord Vader&quot;){ paste(&quot;Bienvenue,&quot;,votre_nom) # nous pourrions aussi écrire : # return(paste(&quot;Bienvenue,&quot;,votre_nom)) } # nous appelons la fonction en tapant son argument entre parenthèses bonjour() ## [1] &quot;Bienvenue, Lord Vader&quot; Vous voyez qu’en ne spécifiant pas l’argument votre_nom, R utilise à la valeur par défaut donnée. Essayez avec votre propre nom maintenant ! Le simple fait de taper le nom de la fonction (sans parenthèses) nous renvoie sa définition, ce qui est parfois pratique : bonjour ## function(votre_nom = &quot;Lord Vader&quot;){ ## paste(&quot;Bienvenue,&quot;,votre_nom) ## # nous pourrions aussi écrire : ## # return(paste(&quot;Bienvenue,&quot;,votre_nom)) ## } Il est instructif de considérer qu’avant de définir la fonction “bonjour” ci-dessus, R ne savait pas quoi faire, si vous aviez appelé bonjour. La fonction n’existait pas ! En ce sens, nous avons appris à R un nouveau truc. Cette fonction permettant de créer de nouvelles capacités en plus d’un langage de base est l’une des caractéristiques les plus puissantes des langages de programmation. En général, il est bon de diviser votre code en plusieurs petites fonctions, plutôt qu’en un long fichier de script. Cela rend votre code plus lisible, et il est plus facile de repérer les erreurs. Les Primers de rstudio.cloud contiennent un tutoriel sur les fonctions qui vous permettra d’aller plus loin sur ce thème. 2.10.4.1 Exercice 6 Écrivez une boucle qui compte à rebours de 10 à 1, en imprimant la valeur de l’itérateur à l’écran. Modifiez cette boucle pour écrire “plus que i itérations” où “i” est l’itérateur Modifiez cette boucle de manière à ce que chaque itération dure environ une seconde. Vous pouvez y parvenir en ajoutant la commande Sys.sleep(1) sous la ligne qui affiche “plus que i itérations”. "],["sum.html", "3 Travailler avec les données 3.1 Statistiques descriptives 3.2 Graphiques 3.3 Statistiques bivariées 3.4 Le tidyverse", " 3 Travailler avec les données Dans ce chapitre, nous allons d’abord apprendre quelques concepts de base qui aident à résumer les données. 3.1 Statistiques descriptives R a intégré des fonctions pour un grand nombre de statistiques descriptives. Pour les variables numériques, nous pouvons résumer les données en examinant leur tendance centrale et leur étendue, par exemple, en utilisant les données de la base mpg déjà chargée dans R. # on chage le paquet ggplot2 qui contient la base de donnée &quot;mpg&quot;. # On l&#39;utilise pour certains graphiques: library(ggplot2) Tendance centrale Supposons que nous voulions connaître la moyenne et la médiane de toutes les valeurs stockées dans la colonne mpg$cty du data.frame : Mesure R Résultat Moyenne mean(mpg$cty) 16.8589744 Médiane median(mpg$cty) 17 Étendue Quelle est la variabilité des valeurs de cette colonne, quelle est son étendue ? Mesure R Résultat Variance var(mpg$cty) 18.1130736 Écart type sd(mpg$cty) 4.2559457 Écart inter quartile IQR(mpg$cty) 5 Minimum min(mpg$cty) 9 Maximum max(mpg$cty) 35 Étendue range(mpg$cty) 9, 35 Variable catégorielles Pour les variables catégorielles, les comptages et les pourcentages peuvent être utilisés pour la statistique descriptive. table(mpg$drv) ## ## 4 f r ## 103 106 25 table(mpg$drv) / nrow(mpg) ## ## 4 f r ## 0.4401709 0.4529915 0.1068376 3.2 Graphiques Maintenant que nous avons des données sur lesquelles travailler et que nous avons appris à les connaître, nos prochaines tâches seront de les visualiser. Souvent, une visualisation correcte peut mettre en évidence des caractéristiques des données qui peuvent servir à une analyse plus approfondie. Nous allons examiner quatre méthodes de visualisation des données en utilisant les fonctions de base de plot intégrées à R : Histogrammes Diagramme en bâtons (Barplots) Boîtes à moustaches (Boxplots) Nuages de points (scatterplots) 3.2.1 Histograms Pour visualiser une seule variable numérique, un histogramme est utile. Il résume la distribution des valeurs dans un tableau. Dans R vous en créez un en utilisant la fonction hist() : hist(mpg$cty) # histogramme de la consommation en ville La fonction hist() comporte un certain nombre de paramètres qui peuvent être modifiés pour rendre notre graphe beaucoup plus agréable. Utilisez l’opérateur ? pour lire la documentation de la fonction hist() et voir la liste complète de ces paramètres. hist(mpg$cty, xlab = &quot;Miles Par Gallon (en ville)&quot;, main = &quot;Histogramme de MPG (City)&quot;, # un titre breaks = 12, # combien de morceaux ? col = &quot;red&quot;, border = &quot;blue&quot;) Il est important de toujours veiller à nommer vos axes et à donner un titre au graphique. L’argument break est spécifique à hist(). La saisie d’un entier donnera une suggestion à R sur le nombre de barres à utiliser pour l’histogramme. Par défaut, R essaiera de deviner intelligemment un bon nombre de break, mais comme nous pouvons le voir ici, il est parfois utile de modifier cela vous-même. 3.2.2 Diagrammes en bâtons Un peu comme un histogramme, un diagramme en bâtons peut fournir un résumé visuel d’une variable catégorielle, ou d’une variable numérique avec un nombre fini de valeurs, comme un classement de 1 à 10. barplot(table(mpg$drv)) # drv indique le type de traction (avant, arrière, 4x4) barplot(table(mpg$drv), xlab = &quot;Traction (f = Avant, r = Arrière, 4 = 4x4)&quot;, ylab = &quot;Fréquence&quot;, main = &quot;Type de traction&quot;, col = &quot;dodgerblue&quot;, border = &quot;darkorange&quot;) 3.2.3 Boîtes à moustaches Pour visualiser la relation entre une variable numérique et une variable catégorielle, on pourrait utiliser une boîte à moustaches (boxplot). Dans la base de données mpg, la variable drv prend un petit nombre fini de valeurs. Une voiture ne peut être qu’à traction avant, à 4 roues motrices ou à traction arrière. unique(mpg$drv) ## [1] &quot;f&quot; &quot;4&quot; &quot;r&quot; Notez tout d’abord que nous pouvons utiliser une boîte à moustaches unique comme alternative à un histogramme pour visualiser une seule variable numérique. Pour ce faire, dans R, nous utilisons la fonction boxplot(). La boîte montre l’ l’écart interquartile, la ligne continue au milieu est la valeur de la médiane, les moustaches indiquent 1,5 fois l’écart interquartile, et les points sont des valeurs aberrantes. boxplot(mpg$hwy) Cependant, nous utiliserons plus souvent des boîtes à moustache pour comparer une variable numérique pour différentes valeurs d’une variable catégorielle. boxplot(hwy ~ drv, data = mpg) Ici, on a utilisé la commande boxplot() pour créer des boîtes à moustache côte à côte. Cependant, comme nous avons maintenant affaire à deux variables, la syntaxe a changé. La syntaxe R hwy ~ drv, data = mpg est la suivante : “Tracez la variable hwy contre la variable drv en utilisant le jeu de données mpg.” Nous voyons l’utilisation d’un argument ~ (qui spécifie une formule) et aussi d’un argument data =. Il s’agit d’une syntaxe commune à de nombreuses fonctions que nous utiliserons régulièrement. boxplot(hwy ~ drv, data = mpg, xlab = &quot;Traction (f = FWD, r = RWD, 4 = 4WD)&quot;, ylab = &quot;Miles Par Gallon (Autoroute)&quot;, main = &quot;MPG (Highway) vs Traction&quot;, pch = 20, cex = 2, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;) Encore une fois, boxplot() a un certain nombre d’arguments supplémentaires qui ont la capacité de rendre notre graphique plus attrayant visuellement. 3.2.4 Nuages de points Enfin, pour visualiser la relation entre deux variables numériques, nous utiliserons un nuage de points. Cela peut être fait avec la fonction plot() et la syntaxe ~ que nous venons d’utiliser avec un boxplot. (La fonction plot() peut également être utilisée de manière plus générale ; voir la documentation pour plus de détails). plot(hwy ~ displ, data = mpg) plot(hwy ~ displ, data = mpg, xlab = &quot;Cylindrée (en litres)&quot;, ylab = &quot;Miles Par Gallon (Autoroute)&quot;, main = &quot;MPG (Highway) vs Cylindrée&quot;, pch = 20, cex = 2, col = &quot;dodgerblue&quot;) 3.2.5 ggplot Toutes les parcelles ci-dessus auraient également pu être générées en utilisant la fonction ggplot du paquet ggplot2 déjà chargé. Vous avez le choix de la fonction que vous utilisez, mais parfois un graphique est plus facile à construire en base R,, parfois l’inverse. ggplot(data = mpg,mapping = aes(x=displ,y=hwy)) + geom_point() Il est impossible de décrire ggplot en termes simples, alors vous pouvez consulter [le site web du paquet] (http://ggplot2.tidyverse.org) qui fournit d’excellents conseils. Nous utiliserons de temps en temps ggplot, afin que vous puissiez vous familiariser avec lui. Montrons rapidement comment on peut personnaliser davantage ce premier graphique : ggplot(data = mpg, mapping = aes(x=displ,y=hwy)) + # ggplot() crée le graphe de base geom_point(color=&quot;blue&quot;,size=2) + # aspect des points scale_y_continuous(name=&quot;Miles Par Gallon (Autoroute)&quot;) + # nom de l&#39;axe des y scale_x_continuous(name=&quot;Cylindrée (in litres)&quot;) + # nom de l&#39;axe des x theme_bw() + # changement de thème couleur ggtitle(&quot;MPG (Autoroute) vs Cylindrée&quot;) # Titre Les graphiques précédents auraient peu être obtenus via ggplot de la façon suivante : ggplot(data=mpg,mapping=aes(x=cty)) + # graphe de base geom_histogram(breaks=c(seq(8,36,2))) + # ajout de l&#39;histogramme avec des points de coupure explicites labs(title = &quot;Histogramme de mpg (cty)&quot;, x =&quot;Miles Par Gallon (en ville)&quot; , y=&quot;Fréquence&quot;) # labels ggplot(data=mpg,mapping=aes(x=drv)) + # graphe de base geom_bar() + # ajour des bâtons scale_x_discrete(&quot;Traction&quot;, labels=c(&quot;4&quot;=&quot;4x4&quot;,&quot;f&quot;=&quot;avant&quot;,&quot;r&quot;=&quot;arrière&quot;)) + # lables des x labs(y=&quot;Fréquence&quot;, title=&quot;Type de traction&quot;) # autres labels ggplot(data=mpg,mapping=aes(x=drv,y=hwy)) + # graphe de base geom_boxplot() + # ajout des boîtes à moustache scale_x_discrete(&quot;Traction&quot;, labels=c(&quot;4&quot;=&quot;4x4&quot;,&quot;f&quot;=&quot;avant&quot;,&quot;r&quot;=&quot;arrière&quot;)) + # labels de x labs(y=&quot;Consommation (mpg, autoroute&quot;,title=&quot;MPG (Highway) vs Traction&quot;) # autres labels 3.3 Statistiques bivariées Nous nous intéressons souvent à la façon dont deux variables sont liées l’une à l’autre. Les concepts fondamentaux sont ici la covariance et la corrélation. Générons des données sur “x” et “y” et traçons les relations entre ces deux variables : Figure 3.1: Quelle est la relation entre \\(x\\) et \\(y\\) ? En prenant comme exemple les données de ce graphique, les concepts de covariance et de corrélation se rapportent au type de question suivante : Étant donné que nous observons une valeur de \\(x=2\\), disons, pouvons-nous nous attendre à une valeur élevée ou faible de \\(y\\), en moyenne ? Quelque chose comme \\(y=2\\) ou plutôt quelque chose comme \\(y=-2\\) ? La réponse à ce type de question peut être apportée en calculant la covariance des deux variables : cov(x,y) ## [1] 1.041195 Ici, cela donne un nombre positif, 1.04, indiquant que, lorsqu’une variable se situe au-dessus de sa moyenne, l’autre l’est aussi. En d’autres termes, cela indique une relation positive. Ce qui est moins clair, cependant, c’est la façon d’interpréter la valeur de 1.04. S’agit-il d’une association positive forte ou faible ? En fait, nous ne pouvons pas le dire. Cela s’explique par le fait que la covariance est mesurée dans les mêmes unités que les données, et que ces unités diffèrent souvent entre les deux variables. Il existe cependant une meilleure mesure à notre disposition, la corrélation, qui est obtenue en standardisant chaque variable. En standardisant une variable \\(x\\) on veut dire diviser \\(x\\) par son écart-type \\(\\sigma_x\\) : \\[ z = \\frac{x}{\\sigma_x} \\] Le coefficient de corrélation entre \\(x\\) et \\(y\\), communément appelé \\(r_{x,y}\\), est alors défini comme \\[ r_{x,y} = \\frac{cov(x,y)}{\\sigma_x \\sigma_y}, \\] et nous nous débarrassons du problème des unités. En R, vous pouvez taper cor(x,y) ## [1] 0.9142495 Maintenant, c’est mieux. Étant donné que la corrélation doit se situer dans \\([-1,1]\\), une valeur de 0.91 est indicative d’une relation positive assez forte pour les données de la figure 3.1 Notez que le fait que \\(x,y\\) soient tirés d’une distribution continue (ils sont distribués selon une loi normale bivariée) n’a aucune implication sur la covariance et la corrélation : Nous pouvons également calculer ces mesures pour des variables aléatoires discrètes. 3.4 Le tidyverse Hadley Wickham est l’auteur des paquets R ggplot2 et aussi de dplyr (et aussi d’une myriade d’autres). Avec ggplot2, il a introduit ce que l’on appelle la grammaire des graphiques (d’où gg) dans R. Une grammaire dans le sens où il y a des noms et des verbes et une syntaxe, c’est-à-dire des règles sur la façon dont les noms et les verbes doivent être mis ensemble pour construire une phrase compréhensible. Il a étendu l’idée de grammaire à divers autres paquets. Le paquet tidyverse est une collection de ces paquets. Les données tidy sont des données où : Chaque variable est une colonne Chaque observation est une ligne Chaque valeur est une cellule On peut dire que c’est une feuille de calcul ordinaire. Et vous avez raison ! Cependant, la plupart du temps, les données nous arrivent pas en ordre, et nous devons d’abord les nettoyer, ou les “ranger”. Une fois qu’elles sont au format “tidy”, nous pouvons utiliser les outils tidyverse avec une grande efficacité pour analyser les données et ne plus nous soucier de savoir quel outil utiliser. 3.4.1 Primers de rstudio.cloud Le site rstudio.cloud propose un ensemble de tutoriels (primers) interactifs de grande qualité basés sur le tidyverse. Après avoir crée un compte gratuit, rendez-vous sur leur page Primers. Vous y trouverez les tutoriels suivants : The Basics Pour avoir une intuition de la façon dont fonctionnent R et les paquets du tidyverse Work with data vous donnera une bonne base de compréhension des tibbles (voir ci-dessous), du paquet dplyr (très utile, en particulier l’opérateur de pipe %&gt;%, voir ci-dessous) Visualize Data fera de vous des maîtres du graphique avec ggplot Tidy Your Data vous initiera à la manipulation plus avancée de base de donnée Les tutoriels Iterate et Write functions seront utiles pour des programmeurs débutants, mais pas forcément nécessaires immédiatement. Au long de nos enseignements, nous sezront amenés à utiliser des commandes du tidyverse, en particulier de dplyr et ggplot2. Nous vous conseillons fortement de vous familiariser avec ces outils en effectuant les tutoriaux. 3.4.2 Importer des données .csv de façon tidy Nous aurions pu utiliser la fonction read_csv() du paquet readr pour lire notre exemple de base de données du chapitre précédent. La fonction readr read_csv() a un certain nombre d’avantages par rapport à la fonction intégrée read.csv. Par exemple, elle est beaucoup plus rapide pour lire des données plus volumineuses. Elle utilise également le paquet tibble pour lire les données comme un tibble. Un tibble est simplement un data.frame qui s’imprime avec une certaine logique. Remarquez dans la sortie ci-dessous que nous avons des informations supplémentaires telles que la dimension et le type de variable. library(readr) # vous devez `install.packages(&quot;readr&quot;)` une seule fois example_data_from_disk = read_csv(&quot;data/example-data.csv&quot;) 3.4.3 Les data.frames “tidy” sont des tibbles Importons quelques données du paquet ggplot2 : data(mpg,package = &quot;ggplot2&quot;) # on charge les données `mpg` depuis le paquet `ggplot2` head(mpg, n = 10) ## # A tibble: 10 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manual… f 21 29 p comp… ## 3 audi a4 2 2008 4 manual… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto(a… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto(l… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manual… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto(a… f 18 27 p comp… ## 8 audi a4 quat… 1.8 1999 4 manual… 4 18 26 p comp… ## 9 audi a4 quat… 1.8 1999 4 auto(l… 4 16 25 p comp… ## 10 audi a4 quat… 2 2008 4 manual… 4 20 28 p comp… La fonction head() affichera les n premières observations de la base de données, comme nous l’avions vu. La fonction head() était plus utile avant les tibbles. Remarquez que “mpg” est déjà un tibble, donc la sortie de “head” indique qu’il n’y a que 10 observations. Notez que cela s’applique à head(mpg, n = 10) et non à mpg lui-même. Notez également que les tibbles impriment un nombre limité de lignes et de colonnes par défaut. La dernière ligne de la sortie imprimée indique que les lignes et les colonnes ont été omises. mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manual… f 21 29 p comp… ## 3 audi a4 2 2008 4 manual… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto(a… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto(l… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manual… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto(a… f 18 27 p comp… ## 8 audi a4 quat… 1.8 1999 4 manual… 4 18 26 p comp… ## 9 audi a4 quat… 1.8 1999 4 auto(l… 4 16 25 p comp… ## 10 audi a4 quat… 2 2008 4 manual… 4 20 28 p comp… ## # … with 224 more rows Regardons également str afin de se familiariser avec le contenu de la base str(mpg) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... Dans cette base de données, une observation porte sur u modèle particulièr d’une voiture une année donnée, et les variables décrivent les attributs de la voiture, par exemple son rendement énergétique sur autoroute. Pour en savoir plus sur l’ensemble de données, nous utilisons l’opérateur ? pour extraire la documentation des données. ?mpg Travailler avec des tibbles est essentiellement la même chose que travailler avec des data.frames simples : names(mpg) ## [1] &quot;manufacturer&quot; &quot;model&quot; &quot;displ&quot; &quot;year&quot; &quot;cyl&quot; ## [6] &quot;trans&quot; &quot;drv&quot; &quot;cty&quot; &quot;hwy&quot; &quot;fl&quot; ## [11] &quot;class&quot; mpg$year ## [1] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 2008 ## [16] 1999 2008 2008 2008 2008 2008 1999 2008 1999 1999 2008 2008 2008 2008 2008 ## [31] 1999 1999 1999 2008 1999 2008 2008 1999 1999 1999 1999 2008 2008 2008 1999 ## [46] 1999 2008 2008 2008 2008 1999 1999 2008 2008 2008 1999 1999 1999 2008 2008 ## [61] 2008 1999 2008 1999 2008 2008 2008 2008 2008 2008 1999 1999 2008 1999 1999 ## [76] 1999 2008 1999 1999 1999 2008 2008 1999 1999 1999 1999 1999 2008 1999 2008 ## [91] 1999 1999 2008 2008 1999 1999 2008 2008 2008 1999 1999 1999 1999 1999 2008 ## [106] 2008 2008 2008 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 2008 ## [121] 2008 2008 2008 2008 1999 1999 2008 2008 2008 2008 1999 2008 2008 1999 1999 ## [136] 1999 2008 1999 2008 2008 1999 1999 1999 2008 2008 2008 2008 1999 1999 2008 ## [151] 1999 1999 2008 2008 1999 1999 1999 2008 2008 1999 1999 2008 2008 2008 2008 ## [166] 1999 1999 1999 1999 2008 2008 2008 2008 1999 1999 1999 1999 2008 2008 1999 ## [181] 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 1999 1999 ## [196] 1999 2008 2008 1999 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 ## [211] 2008 1999 1999 1999 1999 2008 2008 2008 2008 1999 1999 1999 1999 1999 1999 ## [226] 2008 2008 1999 1999 2008 2008 1999 1999 2008 mpg$hwy ## [1] 29 29 31 30 26 26 27 26 25 28 27 25 25 25 25 24 25 23 20 15 20 17 17 26 23 ## [26] 26 25 24 19 14 15 17 27 30 26 29 26 24 24 22 22 24 24 17 22 21 23 23 19 18 ## [51] 17 17 19 19 12 17 15 17 17 12 17 16 18 15 16 12 17 17 16 12 15 16 17 15 17 ## [76] 17 18 17 19 17 19 19 17 17 17 16 16 17 15 17 26 25 26 24 21 22 23 22 20 33 ## [101] 32 32 29 32 34 36 36 29 26 27 30 31 26 26 28 26 29 28 27 24 24 24 22 19 20 ## [126] 17 12 19 18 14 15 18 18 15 17 16 18 17 19 19 17 29 27 31 32 27 26 26 25 25 ## [151] 17 17 20 18 26 26 27 28 25 25 24 27 25 26 23 26 26 26 26 25 27 25 27 20 20 ## [176] 19 17 20 17 29 27 31 31 26 26 28 27 29 31 31 26 26 27 30 33 35 37 35 15 18 ## [201] 20 20 22 17 19 18 20 29 26 29 29 24 44 29 26 29 29 29 29 23 24 44 41 29 26 ## [226] 28 29 29 29 28 29 26 26 26 La sélection de sous-parties de la base est également similaire à la data.frame. Ici, nous trouvons des véhicules à faible consommation de carburant qui font plus de 35 miles par gallon et nous n’affichons que le “manufacturer”, le “model” et “year”. # mpg[condition ligne, condition colonne] mpg[mpg$hwy &gt; 35, c(&quot;manufacturer&quot;, &quot;model&quot;, &quot;year&quot;)] ## # A tibble: 6 x 3 ## manufacturer model year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 honda civic 2008 ## 2 honda civic 2008 ## 3 toyota corolla 2008 ## 4 volkswagen jetta 1999 ## 5 volkswagen new beetle 1999 ## 6 volkswagen new beetle 1999 Une alternative serait d’utiliser la fonction subset() qui a une syntaxe plus lisible. subset(mpg, subset = hwy &gt; 35, select = c(&quot;manufacturer&quot;, &quot;model&quot;, &quot;year&quot;)) Enfin, et de façon plus tidy, nous pourrions utiliser les fonctions filter et select du paquet dplyr qui introduit l’opérateur pipe f(x) %&gt;% g(z) du paquet magrittr. Cet opérateur prend la sortie de la première commande, par exemple y = f(x), et la passe en tant que premier argument à la fonction suivante, c’est-à-dire que nous obtiendrions g(y,z) ici.^[Un pipe est un concept du monde Unix, où il signifie prendre la sortie d’une commande, et la passer à une autre commande. De cette façon, on peut construire une pipeline de commandes. Pour plus d’informations sur l’opérateur de pipe dans R, vous pourriez être intéressé [par ce tutoriel] (https://www.datacamp.com/community/tutorials/pipe-r-tutorial). library(dplyr) mpg %&gt;% filter(hwy &gt; 35) %&gt;% select(manufacturer, model, year) ## # A tibble: 6 x 3 ## manufacturer model year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 honda civic 2008 ## 2 honda civic 2008 ## 3 toyota corolla 2008 ## 4 volkswagen jetta 1999 ## 5 volkswagen new beetle 1999 ## 6 volkswagen new beetle 1999 Notez que la syntaxe ci-dessus est équivalente à la commande suivante sans pipe (qui est beaucoup plus difficile à lire !): library(dplyr) select(filter(mpg, hwy &gt; 35), manufacturer, model, year) ## # A tibble: 6 x 3 ## manufacturer model year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 honda civic 2008 ## 2 honda civic 2008 ## 3 toyota corolla 2008 ## 4 volkswagen jetta 1999 ## 5 volkswagen new beetle 1999 ## 6 volkswagen new beetle 1999 Ces trois approches produisent les mêmes résultats. La méthode que vous utiliserez dépendra largement de la situation donnée ainsi que de vos préférences. 3.4.3.1 Exercice 1 Assurez-vous que l’ensemble de données mpg est chargé en tapant data(mpg) (et library(ggplot2) si ce n’est pas le cas !) Utilisez la fonction table pour savoir combien de voitures ont été construites par mercury ? Quelle est l’année moyenne de construction des voitures dans cet ensemble de données ? Utilisez la fonction mean sur le sous-ensemble de la colonne year qui correspond à “audi”. (Attention : le sous-ensemble d’un “tibble” renvoie un “tibble” (et non un vecteur) !. donc récupérez la colonne year après avoir extrait le sous “tibble”). Utilisez la syntaxe de pipe de dplyr du dessus d’abord avec group_by et ensuite avec summarise(newvar=votre_expression) pour trouver la moyenne year par tous les fabricants (c’est à dire la même que la tâche précédente, mais pour tous les fabricants. n’écrivez pas une boucle !) "],["mcarlo.html", "4 Simulations de Monte-Carlo 4.1 Un premier exemple 4.2 Généralisation : le package MonteCarlo 4.3 Avec une régression par MCO", " 4 Simulations de Monte-Carlo Il est souvent utilse de faire des simulations de Monte-Carlo afin d’étudier les propriétés des estimateurs que nous rencontrerons, et de “visualiser” leurs comprtements. Ces propriétés (biais, convergence, efficacité etc.) sont des propriétés statistques qui ont trait à la distribution des estimateurs, vus comme une variables aléatoire. Une simulation de Monte-Carlo consiste en quelque sortes à tirer plein de valeurs de ces variables aléatoires afin d’étudier leurs distribution. 4.1 Un premier exemple Votre cours de statistrique et de probabilité vous a (normalement) appris que la moyenne d’un échantillon de taille \\(n\\) issu d’une loi Normale \\(N(\\mu,\\sigma^2)\\) de moyenne \\(\\mu\\) et d’écart-type \\(\\sigma\\) est une variable aléatoire suivant une loi normale de moyenne \\(\\mu\\) et d’écart-type \\(\\frac{\\sigma}{\\sqrt{n}}\\). Utilisons R pour le vérifier. Commençons par calculer la moyenne d’un échantillon de 100 tirages d’une \\(N(0,1)\\) n &lt;- 100 mu &lt;- 0 sigma &lt;- 1 x &lt;- rnorm(n,mean=mu,sd=sigma) mean(x) ## [1] -0.1976099 On voit que la moyenne des 100 tirages n’est pas strictement égale à 0, et on ne voit pas bien comment juger de l’écart-type ou de la loi de probabilité de cette moyenne. Si on lance le code une seconde fois, le résultat va d’ailleurs changer : n &lt;- 100 mu &lt;- 0 sigma &lt;- 1 x &lt;- rnorm(n,mean=mu,sd=sigma) mean(x) ## [1] -0.1302389 La raison est que la théorie nous donne les caractéristiques de la distribution de cette moyenne au travers d’un grand nombre d’échantillons de 1000 tirages d’une \\(N(0,1)\\). Nous allons donc répéter le code ci-dessous un grand nombre de fois (\\(K\\) fois), noter à chaque fois la moyenne obtenue, et étudier la distribution de ces \\(K\\) moyennes K &lt;- 10000 # On va faire 10000 réplications, on aura donc 10000 moyennes n &lt;- 100 mu &lt;- 0 sigma &lt;- 1 moyennes &lt;- c() # On crée un vecteur vide qui contiendra les K moyennes for (i in 1:K) { # on initialise la boucle x &lt;- rnorm(n,mean=mu,sd=sigma) # on tire l&#39;ééchantillon moyennes[i] &lt;- mean(x) # on stocke la moyenne issue de la ième réplication à # la ième position du vecteur &quot;moyennes&quot; } moyennes[1:10] # on affiche les 10 premières moyennes ## [1] 0.04769265 -0.05984550 -0.11143053 -0.05447702 0.22417867 0.05566776 ## [7] 0.02042009 -0.02819722 0.03614298 0.14477268 mean(moyennes) # moyenne des moyennes (0 en théorie) ## [1] -0.0009696851 sd(moyennes) # écart-type des moyennes (1/sqrt(n) en théorie) ## [1] 0.09965563 1/sqrt(n) # valeur de 1/sqrt(n) ## [1] 0.1 hist(moyennes,prob=TRUE) # on trace l&#39;histogramme des moyennes lines(density(moyennes),col=&quot;red&quot;) # on y ajoute le tracé de la densité des moyennes Ceux qui ont suivi le tutoriel “Visualize Data” peuvent utiliser les outils graphiques du package ggplot2 library(ggplot2) ggplot(mapping=aes(x=moyennes)) + geom_histogram(aes(y=..density..),fill=&quot;grey&quot;,color=&quot;black&quot;) + geom_density(fill=&quot;blue&quot;,alpha=0.2) NB : le y=..density.. indique à geom_histogram d’utiliser la densité en ordonnées, au lieu de la fréquence. Ça permet d’avoir l’histogramme et la courbe des densité à la même échelle. On constate que 1. La moyenne des K=10000 moyennes est très proche de la moyenne théorique (0) 2. L’écart-type des K moyennes est très proche de l’écart-type théorique (0.1) 3. La distribution est proche de celle d’une loi normale 4.2 Généralisation : le package MonteCarlo L’exemple précédent a montré que l’on pouvait assez facilement effectuer une simulation basique avec du code assez simple. Néanmoins, ce code ne permet de simuler qu’un seul scénario (valeurs de \\(n\\), de \\(\\mu\\) et de \\(\\sigma\\)) à la fois. Si on souhaite voir ce qu’il se passe lorsqu’on fait varier les paramètres de la simulation, il faudrait copier-coller le code de nombreuses fois et modifer à chaque fois les paramètres. De plus, il faudrait faire attention à sauvegarder les résultats dans des vecteurs différents à chaque fois. Afin de s’éviter ces désagréments, le package MonteCarlo permet d’automatiser ces tâches pour nous. On commence par charger le package MonteCarlo ainsi que tidyverse (voir les tutoriels) library(MonteCarlo) # doit avoir package Rcpp installé library(tidyverse) # charge plein d&#39;outils utiles, voir les tutoriels sur rstudio.cloud On définit ensuite une fonction qui va effectuer le tirage de l’échantillon aléatoire, calculer la moyenne, et retourner le résultat : simul_moyenne &lt;- function(n,mu,sigma) { # Notre fonction s&#39;appelle &quot;simul_moyenne&quot; et prend les # arguments n, mu et sigma tirages &lt;- rnorm(n,mean=mu,sd=sigma) # on effectue le tirage aléatoire avec les valeurs données en argument moy &lt;- mean(tirages) # on calcule la moyenne return(list(&quot;moyenne&quot;=moy)) # on la retourne dans une liste nommée &quot;moyenne&quot; } On définit ensuite des vecteurs qui vont définir l’ensemble des “scénarios” de simulations, c’est à dire les valeurs des paramètres que l’ont veut faire varier. Ici on veut deux valeurs pour la taille d’échantillon, deux valeurs pour l’espérance, et trois pour l’écart-type n_grid &lt;- c(10,100) mu_grid &lt;- c(0, 5) sigma_grid &lt;- c(1, 2, 4) On indique ensuite que le programme devra passer toutes les combinaisons de ces valeurs en tant qu’arguments à notre fonction “simul_moyenne” param_list=list(&quot;n&quot;=n_grid, &quot;mu&quot;=mu_grid, &quot;sigma&quot;=sigma_grid) On peut maintenant lancer MonteCarlo en lui indiquant quelle fonction utiliser, le nombre de réplications par scénario, et où trouver la liste des paramètres. On stocke les résultats dans un objet nommé “résultats” resultats&lt;-MonteCarlo(func=simul_moyenne, nrep=10000, param_list=param_list) Regardons ce que l’objet “résultats” contient : summary(resultats) ## Simulation of function: ## ## function(n,mu,sigma) { # Notre fonction s&#39;appelle &quot;simul_moyenne&quot; et prend les ## # arguments n, mu et sigma ## tirages &lt;- rnorm(n,mean=mu,sd=sigma) # on effectue le tirage aléatoire avec les valeurs données en argument ## moy &lt;- mean(tirages) # on calcule la moyenne ## return(list(&quot;moyenne&quot;=moy)) # on la retourne dans une liste nommée &quot;moyenne&quot; ## } ## &lt;bytecode: 0x555d2f408e18&gt; ## ## Required time: 2.31 secs for nrep = 10000 repetitions on 1 CPUs ## ## Parameter grid: ## ## n : 10 100 ## mu : 0 5 ## sigma : 1 2 4 ## ## ## 1 output arrays of dimensions: 2 2 3 10000 On va le transformer les parties pertinentes en data frame (voir les tutoriels) pour une manipulation plus aisée. la fonction head() permet ensuite de lister les premières lignes d’une data frame, afin de vérifier rapidement son contenu. data_resultats &lt;-MakeFrame(resultats) head(data_resultats) ## n mu sigma moyenne ## 1 10 0 1 -0.02827005 ## 2 100 0 1 0.01071684 ## 3 10 5 1 4.98566732 ## 4 100 5 1 4.96968491 ## 5 10 0 2 0.55535111 ## 6 100 0 2 0.36621319 On va maintenant calculer les moyennes et écart-types des 10000 moyennes calculées pour chaque scénario de simulation. On fait appel aux outils de “pipe” (%&gt;%) et de groupe (group_by()) expliqués dans le tutoriel “Work With Data”. On en profite pour ajouter une variable “ecty_theor” donnant l’écart-type théorique de la distribution des moyennes data_resultats %&gt;% mutate(ecty_theor=sigma/sqrt(n)) %&gt;% group_by(n,mu,sigma) %&gt;% summarise(moy=mean(moyenne),ecty=sd(moyenne),ecty_theor=mean(ecty_theor)) ## `summarise()` has grouped output by &#39;n&#39;, &#39;mu&#39;. You can override using the `.groups` argument. ## # A tibble: 12 x 6 ## # Groups: n, mu [4] ## n mu sigma moy ecty ecty_theor ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 0 1 -0.00512 0.319 0.316 ## 2 10 0 2 -0.00368 0.635 0.632 ## 3 10 0 4 0.000882 1.27 1.26 ## 4 10 5 1 5.00 0.314 0.316 ## 5 10 5 2 5.01 0.634 0.632 ## 6 10 5 4 5.00 1.25 1.26 ## 7 100 0 1 0.000331 0.0990 0.1 ## 8 100 0 2 0.00493 0.200 0.2 ## 9 100 0 4 0.00267 0.403 0.4 ## 10 100 5 1 5.00 0.0992 0.1 ## 11 100 5 2 5.00 0.198 0.2 ## 12 100 5 4 5.00 0.401 0.4 On constate que la moyenne des moyenne (colonne “moy”) est très proche de la moyenne théorique (“mu”), de même que l’écart-type de la distribution des moyennes (“ecty”) est très proche de l’écart-type théorique. Nos observations semblent bien coller avec la théorie. On complète l’exercice en faisant un graphique de la fonction de densité estimées de nos résultats, pour chaque valeur de \\(n\\), \\(\\mu\\) et \\(\\sigma\\). ggplot(data=data_resultats,mapping=aes(x=moyenne,group=sigma,color=factor(sigma))) + geom_density() + facet_wrap(mu ~ n,labeller=label_both) 4.3 Avec une régression par MCO R permet de faire une régression linéaire par MCO avec la commande lm()(voir le chapitre sur la répression par MCO) ci-dessous On va charger une base de données préinstallée avec R : les données “mtcars” data(mtcars) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Effectuons une régression linaire de, par exemple, “mpg” sur “cyl” et “disp” lm(data=mtcars,mpg ~ cyl + disp) ## ## Call: ## lm(formula = mpg ~ cyl + disp, data = mtcars) ## ## Coefficients: ## (Intercept) cyl disp ## 34.66099 -1.58728 -0.02058 La sortie est assez minimale. Stockons cette régression dans un objet que nous appellerons “ma_regression” et faisons un summary() de ce dernier ma_regression &lt;- lm(data=mtcars,mpg ~ cyl + disp) summary(ma_regression) ## ## Call: ## lm(formula = mpg ~ cyl + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4213 -2.1722 -0.6362 1.1899 7.0516 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.66099 2.54700 13.609 4.02e-14 *** ## cyl -1.58728 0.71184 -2.230 0.0337 * ## disp -0.02058 0.01026 -2.007 0.0542 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.055 on 29 degrees of freedom ## Multiple R-squared: 0.7596, Adjusted R-squared: 0.743 ## F-statistic: 45.81 on 2 and 29 DF, p-value: 1.058e-09 On a déjà plus de détails. On peut extraire divers éléments issus de lm() : summary(ma_regression)$coef[,&quot;Estimate&quot;] # les beta chapeau ## (Intercept) cyl disp ## 34.66099474 -1.58727681 -0.02058363 summary(ma_regression)$coef[,&quot;Std. Error&quot;] # les écarts-type ## (Intercept) cyl disp ## 2.54700388 0.71184427 0.01025748 summary(ma_regression)$coef[,&quot;Estimate&quot;][&quot;cyl&quot;] # le beta chapeau de la variable &quot;cyl&quot; ## cyl ## -1.587277 summary(ma_regression)$coef[,&quot;Estimate&quot;][[&quot;cyl&quot;]] # la même valeur, mais sans le nom associé, notez les doubles crochets ## [1] -1.587277 On va maintenant utiliser ces éléments pour construire une simulation de Monte-Carlo de la distribution de l’estimateur des MCO \\(\\hat\\beta_x\\) dans un cadre qui respecte les hypothèses de Gauss-Markov library(MonteCarlo) # doit avoir package Rcpp installé library(tidyverse) betareg&lt;-function(n) { x &lt;- rnorm(n,mean=0,sd=1) epsilon &lt;- rnorm(n,mean=0,sd=1) # epsilon suit une loi normale non corrélée à x y &lt;- 1+x+epsilon mareg &lt;- lm(y~x) coeffs &lt;- summary(mareg)$coef[,&quot;Estimate&quot;] betax &lt;- coeffs[[&quot;x&quot;]] return(list(&quot;betax&quot;=betax)) } n_grid&lt;-c(10,100) param_list=list(&quot;n&quot;=n_grid) resultats&lt;-MonteCarlo(func=betareg, nrep=10000, param_list=param_list) summary(resultats) ## Simulation of function: ## ## function(n) { ## x &lt;- rnorm(n,mean=0,sd=1) ## epsilon &lt;- rnorm(n,mean=0,sd=1) # epsilon suit une loi normale non corrélée à x ## y &lt;- 1+x+epsilon ## mareg &lt;- lm(y~x) ## coeffs &lt;- summary(mareg)$coef[,&quot;Estimate&quot;] ## betax &lt;- coeffs[[&quot;x&quot;]] ## return(list(&quot;betax&quot;=betax)) ## } ## &lt;bytecode: 0x555d38074998&gt; ## ## Required time: 17.23 secs for nrep = 10000 repetitions on 1 CPUs ## ## Parameter grid: ## ## n : 10 100 ## ## ## 1 output arrays of dimensions: 2 10000 data_resultats &lt;-MakeFrame(resultats) head(data_resultats) ## n betax ## 1 10 1.4697435 ## 2 100 0.9905716 ## 3 10 1.0533390 ## 4 100 1.1352285 ## 5 10 1.3121881 ## 6 100 1.2713152 data_resultats %&gt;% group_by(n) %&gt;% summarise(moy=mean(betax),ecty=sd(betax)) ## # A tibble: 2 x 3 ## n moy ecty ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 0.998 0.377 ## 2 100 0.999 0.100 ggplot(data=data_resultats,mapping=aes(x=betax,group=n,color=factor(n)))+geom_density() Les résultats ci-dessus sont-ils ceux que l’on s’attendait à avoir ? Que nous dit la théorie ? "],["MCO.html", "5 La régression linéaire par MCO 5.1 La commande de base : lm() et summary() 5.2 Extraire des éléments du modèle 5.3 Graphiques d’analyse des résidus 5.4 Analyse de la variance 5.5 Retour sur summary() 5.6 Les MCO à la main", " 5 La régression linéaire par MCO Dans ce document nous passons rapidement en revue quelques commandes liées à la régression linéaire par MCO dans R. Ce document n’est pas exhaustif, il existe un grand nombre d’outils dans R, nous n’en voyons ici qu’une petite partie. 5.1 La commande de base : lm() et summary() Commençons par ouvrir la base de données mtcars préinstallée dans R, listons ses premières lignes et regardons les statistiques descriptives des variables qui y sont contenues data(&quot;mtcars&quot;) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 summary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 On souhaite faire une régression linéaire par MCO de, par exemple, mpg sur cyl, disp et hp. La commande correspondante est lm(data = mtcars, mpg ~ cyl + disp + hp) : lm(data = mtcars, mpg ~ cyl + disp + hp) ## ## Call: ## lm(formula = mpg ~ cyl + disp + hp, data = mtcars) ## ## Coefficients: ## (Intercept) cyl disp hp ## 34.18492 -1.22742 -0.01884 -0.01468 La sortie est assez minimaliste. On a un rappel de la commande, et le vecteur des \\(\\hat\\beta\\). Comme souvent dans R, la bonne façon de procéder est de sauvegarder le modèle dans un objet sur lequel on va ensuite appliquer des commandes et fonctions pour en extraire les éléments souhaités. Stockons donc ce mdoèle dans un objet nommé “ma_regression” ma_regression &lt;- lm(data=mtcars , mpg ~ cyl + disp + hp) Il n’y a pas de sortie particulière, mais on va maintenant pouvoir manipuler l’objet “ma_regression”. Par exemple, summary(ma_regression) nous donnera par exemple un output bien plus complet de notre régression summary(ma_regression) ## ## Call: ## lm(formula = mpg ~ cyl + disp + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0889 -2.0845 -0.7745 1.3972 6.9183 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.18492 2.59078 13.195 1.54e-13 *** ## cyl -1.22742 0.79728 -1.540 0.1349 ## disp -0.01884 0.01040 -1.811 0.0809 . ## hp -0.01468 0.01465 -1.002 0.3250 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.055 on 28 degrees of freedom ## Multiple R-squared: 0.7679, Adjusted R-squared: 0.743 ## F-statistic: 30.88 on 3 and 28 DF, p-value: 5.054e-09 5.2 Extraire des éléments du modèle On peut obtenir la liste des éléments de l’objet “ma_regression” à l’aide de la fonction names() names(ma_regression) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; On peut accéder à chacun des éléments listés ci-dessus via ma_regression$element ou, dans certains cas via la fonction element(ma_regression) : ma_regression$coefficients ## (Intercept) cyl disp hp ## 34.18491917 -1.22741994 -0.01883809 -0.01467933 coefficients(ma_regression) ## (Intercept) cyl disp hp ## 34.18491917 -1.22741994 -0.01883809 -0.01467933 ma_regression$df.residual ## [1] 28 df.residual(ma_regression) ## [1] 28 On peut à nouveau extraire des éléments individuels de ces éléments. Par exemple, si on souhaite obtenir \\(\\hat\\beta_{cyl}\\), on tapera ma_regression$coefficients[&quot;cyl&quot;] ## cyl ## -1.22742 coefficients(ma_regression)[&quot;cyl&quot;] ## cyl ## -1.22742 Dans les exmples ci-dessus le coefficient obtenu est un vecteur “nommé” (le label “cyl” apparaît). Si on veut extraire juste la valeur numérique, on mettra des doubles crochets : ma_regression$coefficients[[&quot;cyl&quot;]] ## [1] -1.22742 coefficients(ma_regression)[[&quot;cyl&quot;]] ## [1] -1.22742 5.2.1 Données du modèles Il est parfois utile d’avoir accès aux données utilisées pour l’estimation. Si les données présentent des valeurs manquantes, ou que l’on a sélectionné un sous-échantillon particulier, les données utilisées pour le modèle ne seront pas strictement équivalentes aux données de la base “complète” (il est possible que toutes les observations n’y soient pas). On peut accéder aux données via l’élément model : ma_regression$model ## mpg cyl disp hp ## Mazda RX4 21.0 6 160.0 110 ## Mazda RX4 Wag 21.0 6 160.0 110 ## Datsun 710 22.8 4 108.0 93 ## Hornet 4 Drive 21.4 6 258.0 110 ## Hornet Sportabout 18.7 8 360.0 175 ## Valiant 18.1 6 225.0 105 ## Duster 360 14.3 8 360.0 245 ## Merc 240D 24.4 4 146.7 62 ## Merc 230 22.8 4 140.8 95 ## Merc 280 19.2 6 167.6 123 ## Merc 280C 17.8 6 167.6 123 ## Merc 450SE 16.4 8 275.8 180 ## Merc 450SL 17.3 8 275.8 180 ## Merc 450SLC 15.2 8 275.8 180 ## Cadillac Fleetwood 10.4 8 472.0 205 ## Lincoln Continental 10.4 8 460.0 215 ## Chrysler Imperial 14.7 8 440.0 230 ## Fiat 128 32.4 4 78.7 66 ## Honda Civic 30.4 4 75.7 52 ## Toyota Corolla 33.9 4 71.1 65 ## Toyota Corona 21.5 4 120.1 97 ## Dodge Challenger 15.5 8 318.0 150 ## AMC Javelin 15.2 8 304.0 150 ## Camaro Z28 13.3 8 350.0 245 ## Pontiac Firebird 19.2 8 400.0 175 ## Fiat X1-9 27.3 4 79.0 66 ## Porsche 914-2 26.0 4 120.3 91 ## Lotus Europa 30.4 4 95.1 113 ## Ford Pantera L 15.8 8 351.0 264 ## Ferrari Dino 19.7 6 145.0 175 ## Maserati Bora 15.0 8 301.0 335 ## Volvo 142E 21.4 4 121.0 109 On voit que les variables ya apparaissent dans l’ordre dans lesquelles elles ont été spécifiées. Si on veut, par exemple, récupérer le vecteur des observations de \\(y\\) utilisées dans notre estimation, on tapera ma_regression$model[,1] ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 ## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 ## [31] 15.0 21.4 ou ma_regression$model[,&quot;mpg&quot;] ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 ## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 ## [31] 15.0 21.4 5.2.2 Manipulation plus avancée Certaines fonctions supplémentaires sont également disponibles. Par exemple vcov() permet d’obtenir la matrice de variance-covariance des \\(\\hat\\beta\\) vcov(ma_regression) ## (Intercept) cyl disp hp ## (Intercept) 6.712128488 -1.786156956 0.0156477359 0.0069613946 ## cyl -1.786156956 0.635649515 -0.0059597991 -0.0052619916 ## disp 0.015647736 -0.005959799 0.0001082368 -0.0000255242 ## hp 0.006961395 -0.005261992 -0.0000255242 0.0002146479 On peut alors facilement en extraire le vecteur des écart-types des \\(\\hat\\beta\\) : sqrt(diag(vcov(ma_regression))) ## (Intercept) cyl disp hp ## 2.59077758 0.79727631 0.01040369 0.01465087 Parmi les autres fonctions utiles, confint() permet d’obtenir les intervalles de confiance des \\(\\hat\\beta\\) : confint(ma_regression) # tous les coeffs, à 95 % ## 2.5 % 97.5 % ## (Intercept) 28.87795186 39.491886473 ## cyl -2.86056643 0.405726550 ## disp -0.04014908 0.002472913 ## hp -0.04469028 0.015331608 confint(ma_regression,&quot;disp&quot;) # un coeff particulier ## 2.5 % 97.5 % ## disp -0.04014908 0.002472913 confint(ma_regression,&quot;cyl&quot;,level=0.90) # un coeff particulier, à 90 % ## 5 % 95 % ## cyl -2.583691 0.1288515 5.3 Graphiques d’analyse des résidus La fonction plot() permet également de faire automatiquement un certain nombre de “graphes de diagnostiques” de notre modèle plot(ma_regression) Tous ne nous sont pas nécessairement utiles, on peut choisir lequel on souhaite avec l’option which(). Si on ne veut que le graphique des résidus contre les valeurs prédites, on tapera plot(ma_regression,which=1) 5.4 Analyse de la variance La fonction anova() permet d’obtenir un tableau de décomposition de la variance : anvar &lt;- anova(ma_regression) anvar ## Analysis of Variance Table ## ## Response: mpg ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl 1 817.71 817.71 87.6000 4.064e-10 *** ## disp 1 37.59 37.59 4.0274 0.05451 . ## hp 1 9.37 9.37 1.0039 0.32495 ## Residuals 28 261.37 9.33 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ce tableau d’analyse de la variance est “séquentiel” et considère chaque variable prise l’une après l’autre. On peut calculer les RSS, ESS et TSS de la façon suivante. La SCT est la somme des carrés totaux (TSS en anglais). C’est la somme de la seconde colonne du tableau : SCT &lt;- sum(anvar[,2]) SCT ## [1] 1126.047 On peut aussi partir de la définition \\(SCT = \\sum_{i=1}^n(y_i-\\bar y)^2\\) sum((ma_regression$model[,1]-mean(ma_regression$model[,1]))^2) ## [1] 1126.047 La SCR est la somme des carrés des résidus (RSS en anglais). On peut l’obtenir de deux façons : en faisant la somme directement, ou en sélectionnant la dernière entrée de la seconde colonne du tableau d’analyse de la variance : SCR &lt;- sum(ma_regression$residuals^2) SCR ## [1] 261.3694 anvar[nrow(anvar),2] ## [1] 261.3694 Finalement, La somme des carrés expliquée (SCE, ESS en anglais) est donnée par \\(SCT==SCE+SCR\\) et donc \\(SCE=SCT-SCR\\) SCE &lt;- SCT-SCR SCE ## [1] 864.6778 C’est aussi la somme de la seconde colonne du tableau d’anova, à l’exclusion de la dernière entrée sum(anvar[-nrow(anvar),2]) ## [1] 864.6778 C’est également la somme des carrés des valeurs prédites centrées sur leur moyenne : \\(SCE=\\sum_{i=1}^n(\\hat y_i - \\bar{\\hat y})\\) sum((ma_regression$fitted.values-mean(ma_regression$fitted.values))^2) ## [1] 864.6778 5.4.1 \\(R^2\\) et \\(\\hat\\sigma\\) On voit que les éléments de l’objet issu de lm() ne comprennent ni le \\(R^2\\) ni l’estimateur de la variance des résidus. On peut y accéder avec summary() (voir plus loin), mais on peut également les calculer à la main : \\(R^2=\\frac{SCR}{SCT}=1-\\frac{SCR}{SCT}\\) R2 &lt;- SCE/SCT R2 ## [1] 0.7678877 1-SCR/SCT ## [1] 0.7678877 \\(\\hat\\sigma=\\sqrt{\\frac{SCR}{n-K-1}}\\). Le nombre d’observations \\(n\\) est donné par la fonction nobs(), et \\(K+1\\) est le “rank” listé dans “ma_regression$rank” sigmachap &lt;- sqrt(SCR/(nobs(ma_regression)-ma_regression$rank)) sigmachap ## [1] 3.055261 5.5 Retour sur summary() Souvenons nous que summary(ma_regression) nous donnait un tableau contenant pas mal d’éléments que nous venons de calculer “à la main”, comme par exemple les écarts-type des \\(\\hat\\beta\\) summary(ma_regression) ## ## Call: ## lm(formula = mpg ~ cyl + disp + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0889 -2.0845 -0.7745 1.3972 6.9183 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.18492 2.59078 13.195 1.54e-13 *** ## cyl -1.22742 0.79728 -1.540 0.1349 ## disp -0.01884 0.01040 -1.811 0.0809 . ## hp -0.01468 0.01465 -1.002 0.3250 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.055 on 28 degrees of freedom ## Multiple R-squared: 0.7679, Adjusted R-squared: 0.743 ## F-statistic: 30.88 on 3 and 28 DF, p-value: 5.054e-09 On peut également sauvegarder cet objet afin d’en extraire des éléments : resum_ma_regression &lt;- summary(ma_regression) names(resum_ma_regression) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; On peut alors utiliser ces éléments. Attention cependant, l’élément “coefficients” issu de summary()' contient le *tableau complet* de résultats, alors que celui issu delm()` ne contient que le vecteur de résultats resum_ma_regression$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.18491917 2.59077758 13.194849 1.537198e-13 ## cyl -1.22741994 0.79727631 -1.539516 1.349044e-01 ## disp -0.01883809 0.01040369 -1.810711 8.092901e-02 ## hp -0.01467933 0.01465087 -1.001943 3.249519e-01 ma_regression$coefficients ## (Intercept) cyl disp hp ## 34.18491917 -1.22741994 -0.01883809 -0.01467933 On peut retrouver les valeurs du \\(R^2\\) et de \\(\\hat\\sigma\\) calculées à la main ci dessus resum_ma_regression$r.squared ## [1] 0.7678877 resum_ma_regression$sigma ## [1] 3.055261 5.6 Les MCO à la main On peut bien entendu “s’amuser” à coder nous même l’estimateur des MCO, la décomposition de la variance etc. Le but est de s’entraîner et de s’assurer qu’on comprends bien d’où sortent les valeurs fournies par lm(). On va utiliser les formules standard des MCO : \\(\\hat\\beta=(X&#39;X)^{-1}X&#39;y\\) \\(\\hat y=X\\hat\\beta\\) \\(e=y-\\hat y\\) \\(SCR=\\sum_{i=1}^ne_i^2\\) \\(\\hat \\sigma = \\frac{SCR}{n-K-1}\\) \\(SCT=\\sum_{i=1}^n(y_i-\\bar y)\\) \\(R^2=1-\\frac{SCR}{SCT}\\) \\(\\widehat{Var}(\\hat\\beta)=\\hat\\sigma^2(X&#39;X)^{-1}\\) y &lt;- ma_regression$model[,&quot;mpg&quot;] # on extrait y X &lt;- as.matrix(ma_regression$model[,c(&quot;cyl&quot;,&quot;disp&quot;,&quot;hp&quot;)]) # On extrait la matrice X X &lt;- cbind(&quot;(intercept)&quot;=rep(1,nrow(X)),X) # On y ajoute la constante XpXinv &lt;- solve(t(X) %*% X) # Clacul de (X&#39;X)^{-1} Xpy &lt;- t(X) %*% y # Calcul de X&#39;y mes_betachap &lt;- XpXinv %*% Xpy # calcul de beta chapeau mes_betachap ## [,1] ## (intercept) 34.18491917 ## cyl -1.22741994 ## disp -0.01883809 ## hp -0.01467933 # prédiction des résidus mes_ychap &lt;- X %*% mes_betachap mes_residus &lt;- y-mes_ychap # calcul de SCR et SCT ma_SCR &lt;- sum(mes_residus^2) ma_SCT &lt;- sum((y-mean(y))^2) # calcul du R2 mon_R2 &lt;- 1-ma_SCR/ma_SCT # calcul de sigma chapeau mon_sigmachap &lt;- sqrt(ma_SCR/(length(y)-ncol(X))) # affichage des résultats c(&quot;SCR&quot;=ma_SCR,&quot;SCT&quot;=ma_SCT,&quot;R carré&quot;=mon_R2,&quot;sigma chapeau&quot;=mon_sigmachap) ## SCR SCT R carré sigma chapeau ## 261.3693529 1126.0471875 0.7678877 3.0552610 # matrice de variance-covariance des beta chapeau ma_varcov &lt;- (mon_sigmachap^2) * XpXinv # vecteur des écart-types mes_ecty &lt;- sqrt(diag(ma_varcov)) # vecteur des t-stats mes_tstats &lt;- mes_betachap/mes_ecty # mise en forme du tableau de résultats mon_tableau &lt;- cbind(mes_betachap,mes_ecty,mes_tstats) colnames(mon_tableau)=c(&quot;Coeff.&quot;, &quot;Ec. ty.&quot;,&quot;t-stat&quot;) mon_tableau ## Coeff. Ec. ty. t-stat ## (intercept) 34.18491917 2.59077758 13.194849 ## cyl -1.22741994 0.79727631 -1.539516 ## disp -0.01883809 0.01040369 -1.810711 ## hp -0.01467933 0.01465087 -1.001943 "],["hetero.html", "6 Traitement de l’hétéroscédasticité 6.1 Introduction 6.2 Détection de l’hétéroscédasticité 6.3 Correction de White 6.4 MCQG par pondération", " 6 Traitement de l’hétéroscédasticité 6.1 Introduction Les hypothèses habituelles des MCO supposent que les termes d’erreur \\(\\epsilon_i\\) sont homoscédastiques, c’est-à-dire que leur variance est constante, et ne dépend pas des caractéristiques individuelles : \\(Var(\\epsilon_i)=\\sigma^2~\\forall i\\). Elles supposent également que les covariances entre termes d’erreur sont nulles : \\(Cov(\\epsilon_i,\\epsilon_j)=0~\\forall i\\neq j\\). Ces deux hypothèses (dites de “sphéricité” des termes d’erreur) assurent que l’estimateur des MCO est de variance minimale parmi les estimateurs linéaires sans biais (MCO est BLUE) ; et permet un mode de calcul simple de la matrice de variance-covariance des \\(\\hat\\beta\\) : \\(V(\\hat\\beta)=\\sigma^2(X&#39;X)^{-1}\\). L’hétéroscédasticité est la violation de l’hypothèse d’homoscédasticité. On parle d’hétéroscédasticité lorsque la variance des termes d’erreur n’est pas constante : \\(Var(\\epsilon_i)=\\sigma^2_i\\). Dans cette section, on continue à considérer que les termes d’erreurs ne sont pas corrélés entre eux. 6.2 Détection de l’hétéroscédasticité On détecte la présence d’hétéroscédasticité à l’aide d’analyse graphique d’une part, et de tests statistiques d’autre part. Commençons par charger des données, et faire une régression dont on analysera les résultats. La base de données “TableF9-2.csv” contient 100 observations sur la dépense en logement (avgexp), l’age, la statut d’occupation (ownrent) et le revenu (income) et son carré (income2). On va régresser la dépense en logement sur ces variables, et étudier la présence d’hétéroscédasticité library(readr) ma_base &lt;- read_csv(&quot;https://raw.githubusercontent.com/ATerracol/P8Econ/master/data/TableF9-2.csv&quot;) modele &lt;- lm(data=ma_base,avgexp ~ age + ownrent + income + income2) smodele &lt;- summary(modele) smodele ## ## Call: ## lm(formula = avgexp ~ age + ownrent + income + income2, data = ma_base) ## ## Residuals: ## Min 1Q Median 3Q Max ## -426.40 -120.82 -38.30 56.95 1596.32 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -115.991 157.831 -0.735 0.4642 ## age -3.654 3.752 -0.974 0.3326 ## ownrent 60.881 61.949 0.983 0.3282 ## income 156.467 63.954 2.447 0.0163 * ## income2 -9.076 6.202 -1.463 0.1467 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 272.2 on 95 degrees of freedom ## Multiple R-squared: 0.1788, Adjusted R-squared: 0.1443 ## F-statistic: 5.173 on 4 and 95 DF, p-value: 0.0008179 6.2.1 Analyse graphique des résidus Les résidus de la régression étant des estimations des termes d’erreur, on se base sur ces résidus pour étudier la présence d’hétéroscédasticité # graphique des résidus contre valeurs prédites plot(modele,which=1 ) # graphique des résidus contre la variable explicative &quot;income&quot; plot(ma_base$income,resid(modele)) # Idem, avec ggplot2 library(ggplot2) ggplot(data=ma_base) + geom_point(aes(x=income,y=residuals(modele))) On voit que, si le graphique des résidus contre les valeurs prédites n’est pas forcément très clair, celui contre la variable “income” semble indiquer la présence d’hétéroscédasticité : la variance des résidus semble augmenter avec les valeurs de la variable de revenu “income”. Celà semble logique dans la mesure où un niveau de revenu plus élevé donnee accès à une gamme de logement de prix plus varié, et donc on s’attend à ce que la variation des dépenses de logement soit plus forte pour les revenus les plus élevés, même à âge et à statut d’occupation donné. 6.2.2 Test de Breusch-Pagan Le test de Breusch-Pagan est un test statistique permettant de détecter la présence d’hétéroscédasticité sous l’hypothèse que \\(Var(\\epsilon_i)=\\sigma^2(\\alpha_0+\\boldsymbol{\\alpha z})\\) où \\(\\boldsymbol{z}\\) est un vecteur de variable à la source de l’hétéroscédasticité. L’hypothèse nulle est que \\(\\boldsymbol{\\alpha}=\\boldsymbol{0}\\), c’est-à-dire l’absence d’hétéroscédasticité. On peut effectuer ce test à l’aide de la commande bptest issue du paquet lmtest library(lmtest) bptest(modele) ## ## studentized Breusch-Pagan test ## ## data: modele ## BP = 7.2289, df = 4, p-value = 0.1243 Ici, le test n’est pas très concluant (p-value &gt;0.1) Par contre, en spécifiant qu’on se concentre sur les variables de revenu pour tester l’hétéroscédasticité (avec l’option varformula) : bptest(modele, varformula = ~ income + income2, data=ma_base) ## ## studentized Breusch-Pagan test ## ## data: modele ## BP = 6.813, df = 2, p-value = 0.03316 on rejette maintenant \\(H0\\) au niveau \\(\\alpha=5 \\%\\) et on conclut à la présence d’hétéroscédasticité. 6.3 Correction de White Une façon de corriger de la présence d’hétéroscédasticité dans un modèle est non pas de chercher à rendre les termes d’erreur sphérique, mais de chercher à calculer correctement la matrice de variance-covariance des \\(\\hat\\beta\\). La méthode usuelle est celle de White, dans sa variante dite “HC1”. Il existe plusieurs paquets permettant d’effectuer cette correction dans R. nous en présentons quelques uns ici : Commençons par estimer le modèle sans correction, sauvegardons le dans un objet nommé modele, et affichons son summary : modele &lt;- lm(data=ma_base,avgexp ~ age + ownrent + income + income2) smodele &lt;- summary(modele) smodele ## ## Call: ## lm(formula = avgexp ~ age + ownrent + income + income2, data = ma_base) ## ## Residuals: ## Min 1Q Median 3Q Max ## -426.40 -120.82 -38.30 56.95 1596.32 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -115.991 157.831 -0.735 0.4642 ## age -3.654 3.752 -0.974 0.3326 ## ownrent 60.881 61.949 0.983 0.3282 ## income 156.467 63.954 2.447 0.0163 * ## income2 -9.076 6.202 -1.463 0.1467 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 272.2 on 95 degrees of freedom ## Multiple R-squared: 0.1788, Adjusted R-squared: 0.1443 ## F-statistic: 5.173 on 4 and 95 DF, p-value: 0.0008179 6.3.1 Les paquets lmtest et sandwich le paquet sandwich fournit la commande vcovHCqui corrige la matrice de variance covariance, tandis que le paquet lmtest fournit la commande coeftest qui permet d’utiliser cette dernière pour construire un tableau de résultat corrigé. library(lmtest) library(sandwich) modele_robust &lt;- coeftest(modele, vcov = vcovHC(modele, type = &quot;HC1&quot;)) modele_robust ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -115.9914 151.9929 -0.7631 0.4473 ## age -3.6537 2.4463 -1.4936 0.1386 ## ownrent 60.8815 67.8642 0.8971 0.3719 ## income 156.4672 73.0671 2.1414 0.0348 * ## income2 -9.0760 6.1422 -1.4776 0.1428 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On constate que si les \\(\\hat\\beta\\) sont identiques, les écarts-type et toute l’inférence est modifiée 6.3.2 Les paquets lmtest et car le paquet car fournit la commande hccn qui corrige la matrice de variance covariance, tandis que le paquet lmtest fournit la commande coeftest qui permet d’utiliser cette dernière pour construire un tableau de résultat corrigé. library(lmtest) library(car) modele_robust2 &lt;- coeftest(modele, vcov = hccm(modele, type = &quot;hc1&quot;)) modele_robust2 ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -115.9914 151.9929 -0.7631 0.4473 ## age -3.6537 2.4463 -1.4936 0.1386 ## ownrent 60.8815 67.8642 0.8971 0.3719 ## income 156.4672 73.0671 2.1414 0.0348 * ## income2 -9.0760 6.1422 -1.4776 0.1428 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On constate que les deux paquets donnent des résultats identiques 6.3.3 Le paquet estimatr Le paquet estimatr, plus récent, permet d’estimer directement des modèles corrigés (sans avoir à estimer le modèle non corrigé au préalable). Il faut utiliser la commande lm_robust() avec l’option se_type=\"HC1\" : library(estimatr) modele_robust3 &lt;- lm_robust(data=ma_base,avgexp ~ age + ownrent + income + income2, se_type = &quot;HC1&quot;) summary(modele_robust3) ## ## Call: ## lm_robust(formula = avgexp ~ age + ownrent + income + income2, ## data = ma_base, se_type = &quot;HC1&quot;) ## ## Standard error type: HC1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) -115.991 151.993 -0.7631 0.4473 -417.74 185.753 95 ## age -3.654 2.446 -1.4936 0.1386 -8.51 1.203 95 ## ownrent 60.881 67.864 0.8971 0.3719 -73.85 195.609 95 ## income 156.467 73.067 2.1414 0.0348 11.41 301.524 95 ## income2 -9.076 6.142 -1.4776 0.1428 -21.27 3.118 95 ## ## Multiple R-squared: 0.1788 , Adjusted R-squared: 0.1443 ## F-statistic: 6.323 on 4 and 95 DF, p-value: 0.0001485 Les résultats sont à nouveau identiques, mais le paquet estimatr fournit en plus les intervalles de confiance dans le tableau de résultat. Pour les obtenir avec les autres paquets, il aurait fallu taper confint(modele_robust) ## 2.5 % 97.5 % ## (Intercept) -417.735526 185.752639 ## age -8.510198 1.202751 ## ownrent -73.845951 195.608913 ## income 11.410587 301.523775 ## income2 -21.269821 3.117847 confint(modele_robust2) ## 2.5 % 97.5 % ## (Intercept) -417.735526 185.752639 ## age -8.510198 1.202751 ## ownrent -73.845951 195.608913 ## income 11.410587 301.523775 ## income2 -21.269821 3.117847 6.4 MCQG par pondération La méthode des moindres carrés généralisée permet en principe de corriger l’hétéroscédasticité en pondérant les observations par l’inverse de l’écart-type du terme d’erreur. En posant pour hypothèse que \\(\\sigma^2_i=\\exp(\\boldsymbol{x&#39;_i\\alpha})\\), on peut estimer \\(\\boldsymbol{\\hat\\alpha}\\) en régressant les logs carrés des résidus sur les \\(x\\), puis en calculant \\(\\hat\\alpha_i^2=\\exp(\\boldsymbol{x&#39;_i\\hat\\alpha})\\) et en pondérant les observations par \\(\\hat\\sigma_i\\) : modele &lt;- lm(data=ma_base,avgexp ~ age + ownrent + income + income2) ma_base$logresid2 &lt;- log(residuals(modele)^2) # on ajoute la variable ln(e^2) modeleresid &lt;- lm(data=ma_base,logresid2 ~ age + ownrent + income + income2) # on les régresse sur les x ma_base$e2chap &lt;- exp(modeleresid$fitted.values) # on calcule la variance prédite modele_mcqg &lt;- lm(data=ma_base,avgexp ~ age + ownrent + income + income2,weight=1/e2chap) # on pondère la régression par 1/variance summary(modele_mcqg) ## ## Call: ## lm(formula = avgexp ~ age + ownrent + income + income2, data = ma_base, ## weights = 1/e2chap) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -3.3337 -1.3200 -0.5003 0.6167 10.1564 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -33.486 84.332 -0.397 0.6922 ## age -3.214 2.200 -1.461 0.1474 ## ownrent 44.687 42.124 1.061 0.2914 ## income 108.266 41.342 2.619 0.0103 * ## income2 -4.605 4.014 -1.147 0.2542 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.138 on 95 degrees of freedom ## Multiple R-squared: 0.2432, Adjusted R-squared: 0.2113 ## F-statistic: 7.632 on 4 and 95 DF, p-value: 2.242e-05 On constate que les résultats diffèrent de ceux de la correction de White. NB, l’aide de lm(), accessible par ?lm, précise que “weights can be used to indicate that different observations have different variances (with the values in weights being inversely proportional to the variances)” : on indique donc \\(\\frac{1}{\\hat\\sigma^2_i}\\) dans l’option weight On peut aussi pondérer “à la main” les variables pour implémenter les MCQG. à l’aide de la variable e2chap crée ci dessus, on peut diviser toutes les variables par \\(\\hat\\sigma_i\\) ma_base$avgexp_corr &lt;- ma_base$avgexp/sqrt(ma_base$e2chap) ma_base$age_corr &lt;- ma_base$age/sqrt(ma_base$e2chap) ma_base$ownrent_corr &lt;- ma_base$ownrent/sqrt(ma_base$e2chap) ma_base$income_corr &lt;- ma_base$income/sqrt(ma_base$e2chap) ma_base$income2_corr &lt;- ma_base$income2/sqrt(ma_base$e2chap) ma_base$newcons=1/sqrt(ma_base$e2chap) # On pondère aussi la constante modele_mcqg2 &lt;- lm(data=ma_base,avgexp_corr ~ newcons + age_corr + ownrent_corr + income_corr + income2_corr +0) # le +0 supprime la constante summary(modele_mcqg2) ## ## Call: ## lm(formula = avgexp_corr ~ newcons + age_corr + ownrent_corr + ## income_corr + income2_corr + 0, data = ma_base) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3337 -1.3200 -0.5003 0.6167 10.1564 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## newcons -33.486 84.332 -0.397 0.6922 ## age_corr -3.214 2.200 -1.461 0.1474 ## ownrent_corr 44.687 42.124 1.061 0.2914 ## income_corr 108.266 41.342 2.619 0.0103 * ## income2_corr -4.605 4.014 -1.147 0.2542 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.138 on 95 degrees of freedom ## Multiple R-squared: 0.4559, Adjusted R-squared: 0.4273 ## F-statistic: 15.92 on 5 and 95 DF, p-value: 2.267e-11 Les deux méthodes donnent bien entendu des résultats identiques. Sauf si on a de bonnes raisons, il est conseillé de s’en tenir à la correction de White, pour laquelle plusieurs variantes sont accessibles (HC0, HC1, HC2 et HC3). "],["dsls.html", "7 Variables instrumentales et doubles moindres carrés 7.1 Introduction 7.2 La méthode des variables instrumentales/doubles moindres carrés 7.3 Implémentation sous R : la commande ivreg 7.4 Tests de diagnostics 7.5 2SLS à la main", " 7 Variables instrumentales et doubles moindres carrés 7.1 Introduction Une hypothèse centrale des MCO dans le modèle \\(y=X\\beta+\\epsilon\\) est celle de l’exogénéité des variables explicatives : \\(E(\\boldsymbol{\\epsilon}|\\boldsymbol{X})=\\boldsymbol{0}\\). Cette hypothèse assure que l’estimateur des MCO est sans biais, et que l’on peut donc l’interpréter comme l’effet causal de la variable explicative sur \\(y\\). Si cette hypothèse n’est pas valide, alors l’estimateur des MCO est biaisé, et son interprétation ne peut plus être causale. Un solution potentielle à ce problème d’endogénéité est d’utiliser des variables externes au modèles, dites variables instrumentales afin de “filtrer” l’endogénéité des variables problématique et de ne conserver que la partie qui est non corrélée aux termes d’erreur \\(\\epsilon\\). Pour être valide, les variables instrumentales \\(Z\\) doivent respecter deux conditions : Une condition d’orthogonalité qui stipule qu’elles sont non corrélées aux termes d’erreur : \\(\\mathrm{plim} Z&#39;\\epsilon=0\\) Une condition de pertinence (ou de rang) qui stiplule qu’elles sont corrélées aux variables explicatives \\(X\\) (exogènes comme endogènes) : \\(\\mathrm{plim} \\frac{1}{n}Z&#39;X =Q\\) où \\(Q\\) est une matrice inversible. Intuitivement, une variable instrumentale ne doit pas avoir d’influence directe sur \\(y\\), sauf via son effet sur la variable explicative endogène. 7.2 La méthode des variables instrumentales/doubles moindres carrés Supposons un modèle de la forme \\(y=X\\beta + \\epsilon\\) où \\(X\\) peut se décomposer en \\(X=[D \\quad G]\\) où \\(D\\) contient des variables explicatives exogènes, et \\(G\\) des variables explicatives endogènes. Soit \\(I\\) une matrice contenant les instruments externes. Notons \\(Z =[X\\quad I]\\) la matrice contenant les explicatives exogène et les instruments externes. Le modèle sera dit “juste identifié” si le nombre de variables dans \\(I\\) est égal au nombre de variables dans \\(G\\) (le nombre d’instruments externe est égal au nombre de variables explicatives endogènes). Il sera dit “sur-identifié” si le nombre de variables dans \\(I\\) est supérieur au nombre de variables dans \\(G\\) (le nombre d’instruments externe est supérieur au nombre de variables explicatives endogènes). Il est possible de tester la validité des instruments dans un modèle sur identifié, mais pas dans un modèle juste identifié. Afin de “purger” les variables explicatives endogène de leur corrélation avec le terme d’erreur, on effectue la régressions la régression des variables explicatives \\(X\\) sur \\(Z\\) les instruments externes et les explicatives exogènes. On en dire ensuite les valeurs prédites \\(\\hat X\\). Les variables dans \\(\\hat X\\) sont orthogonales aux termes d’erreur \\(\\epsilon\\) car elles sont des combinaisons linéaires de variables indépendantes des termes d’erreur par hypothèse (que ça soit les explicatives exogènes ou les instruments externes) ; et leurs corrélation avec \\(X\\) est maximisée par la procédure de régression. Les variables \\(\\hat X\\) sont donc des versions “filtrées” de \\(X\\), où on a retiré toutle la partie corrélée avec \\(\\epsilon\\) En remplaçant ensuite \\(X\\) par \\(\\hat X\\) dans le modèle d’intérêt, on obtient des coefficients non biaisés. La matrice de variance-covariance doit par contre être calculée avec une formule adaptée, celle issue des MCO ne prenant pas en compte l’étape d’instrumentation. 7.3 Implémentation sous R : la commande ivreg En pratique on va utiliser la commande ivreg() issue du paquet AER ou du paquet ivreg On commence par importer des données library(readr) # Pour importer des données educwages &lt;- read_csv(&quot;https://raw.githubusercontent.com/ATerracol/P8Econ/master/data/educwages.csv&quot;) Cette base contient 1000 observations, et 5 variables : wages : salaire union : affiliation à un syndicat education : niveau d’éducation meducation : niveau d’éducation de la mère (mother’s education) feducation : niveau d’éducation du père (father’s education) Le but est d’estimer l’effet causal de l’éducation sur le salaire, en contrôlant pour l’appartenance syndicale On commence par un modèle naïf en régressant le niveau de salaire sur le niveau d’éducation et l’appartenance syndicale modele_naif &lt;- lm(data=educwages, wages ~ education + union) summary(modele_naif) ## ## Call: ## lm(formula = wages ~ education + union, data = educwages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6095 -0.6447 0.0160 0.6294 3.2218 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.73773 0.23517 117.95 &lt;2e-16 *** ## education 1.14516 0.01439 79.57 &lt;2e-16 *** ## unionYes 1.96288 0.06023 32.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9514 on 997 degrees of freedom ## Multiple R-squared: 0.878, Adjusted R-squared: 0.8778 ## F-statistic: 3588 on 2 and 997 DF, p-value: &lt; 2.2e-16 On pense que le niveau d’éducation est sans doute endogène dans ce modèle. Il est en effet sans doute corrélé à des caractéristiques inobservées qui influencent le salaire (et qui sont donc dans le terme d’erreur) : capacité de travail, intelligence, etc. On veut donc corriger l’endogénéité par la techniques des variables instrumentales/2SLS. Les instruments externes potentiels sont les niveaux éducation des parents (meducation et feducation) Idée : Hypothèse de pertinence (hypothèse de rang) : L’éducation des parents est corrélée à l’éducation des enfants (via une meilleure connaissance du système scolaire, une aide aux devoirs, etc.) Hypothèse d’orthogonalité : l’éducation des parents ne joue pas directement sur le niveau de salaire des enfants (on exclut la possibilité de faire jouer le “piston” pour que ses enfants obtiennent un meilleur poste et donc un meilleur salaire) On effectue donc la régression 2SLS à l’aide la commande ivreg() issu du paquet AER library(AER) # commande ivreg # syntaxe : ivreg(y ~ les variables dans X | variables dans Z) # ivreg(y ~ liste des explicatives, exogènes et endogène | liste des explicatives exogènes et des instruments externes) modele_iv &lt;- ivreg(data=educwages, wages ~ education + union | union + meducation + feducation) # NB : modèle sur-identifié : 1 explicative endogène (education) et 2 instruments externes (meducation et feducation) summary(modele_iv) ## ## Call: ## ivreg(formula = wages ~ education + union | union + meducation + ## feducation, data = educwages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.686710 -0.698687 -0.004686 0.724659 3.413646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.55263 0.28867 105.84 &lt;2e-16 *** ## education 0.97005 0.01774 54.70 &lt;2e-16 *** ## unionYes 1.93018 0.06457 29.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.02 on 997 degrees of freedom ## Multiple R-Squared: 0.8599, Adjusted R-squared: 0.8596 ## Wald test: 1864 on 2 and 997 DF, p-value: &lt; 2.2e-16 On constate une baisse sensible de l’effet estimé de l’éducation par rapport à l’estimation “naïve” par MCO 7.4 Tests de diagnostics Plusieurs tests de disgnostic sont disponibles après ivreg() (présence d’instruments faibles, test d’endogénéité, test de suridentification). On les obtient avec l’option diagnostics=TRUE du summary() du modèle issu de ivreg() summary(modele_iv, diagnostics = TRUE) ## ## Call: ## ivreg(formula = wages ~ education + union | union + meducation + ## feducation, data = educwages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.686710 -0.698687 -0.004686 0.724659 3.413646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.55263 0.28867 105.84 &lt;2e-16 *** ## education 0.97005 0.01774 54.70 &lt;2e-16 *** ## unionYes 1.93018 0.06457 29.89 &lt;2e-16 *** ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 2 996 1544.878 &lt;2e-16 *** ## Wu-Hausman 1 996 850.772 &lt;2e-16 *** ## Sargan 1 NA 0.127 0.721 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.02 on 997 degrees of freedom ## Multiple R-Squared: 0.8599, Adjusted R-squared: 0.8596 ## Wald test: 1864 on 2 and 997 DF, p-value: &lt; 2.2e-16 Les tests présentés sont les suivants : Weak instruments : test d’instruments faibles. \\(H0\\) : les instruments sont faibles (on veut \\(F&gt;10\\)) Wu-Hausman : test d’endogénéité : \\(H0\\) : mon explicative est exogène (inutile d’instrumenter) Sargan : test de suridentification : \\(H0\\) mes instruments sont exogènes 7.5 2SLS à la main On peut effectuer les doubles moindres carrés “à la main” afin de vérifier notre compréhension de la procédure, et également dans le but d’examiner la régression de première étape #Première étape : régresser l&#39;explicative endogène sur les explicatives exogènes et les instruments externes first_stage &lt;- lm(data=educwages, education ~ union + meducation + feducation ) summary(first_stage) ## ## Call: ## lm(formula = education ~ union + meducation + feducation, data = educwages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3555 -0.7034 0.0306 0.7075 3.4211 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.43385 0.23202 14.800 &lt;2e-16 *** ## unionYes -0.08712 0.06544 -1.331 0.183 ## meducation 0.48411 0.01254 38.603 &lt;2e-16 *** ## feducation 0.48669 0.01262 38.577 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.034 on 996 degrees of freedom ## Multiple R-squared: 0.7567, Adjusted R-squared: 0.756 ## F-statistic: 1033 on 3 and 996 DF, p-value: &lt; 2.2e-16 # On calcule l&#39;endogène prédite, on l&#39;intègre à la base de donnée educwages$educ_chapeau &lt;- first_stage$fitted.values # seconde étape : remplace education par education prédite dans le modèle principal second_stage &lt;- lm(data=educwages, wages ~ educ_chapeau + union) summary(second_stage) ## ## Call: ## lm(formula = wages ~ educ_chapeau + union, data = educwages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.6102 -1.2353 -0.0281 1.2688 6.7323 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.5526 0.5323 57.40 &lt;2e-16 *** ## educ_chapeau 0.9700 0.0327 29.66 &lt;2e-16 *** ## unionYes 1.9302 0.1191 16.21 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.88 on 997 degrees of freedom ## Multiple R-squared: 0.5237, Adjusted R-squared: 0.5228 ## F-statistic: 548.1 on 2 and 997 DF, p-value: &lt; 2.2e-16 # On compare avec ivreg summary(modele_iv) ## ## Call: ## ivreg(formula = wages ~ education + union | union + meducation + ## feducation, data = educwages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.686710 -0.698687 -0.004686 0.724659 3.413646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.55263 0.28867 105.84 &lt;2e-16 *** ## education 0.97005 0.01774 54.70 &lt;2e-16 *** ## unionYes 1.93018 0.06457 29.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.02 on 997 degrees of freedom ## Multiple R-Squared: 0.8599, Adjusted R-squared: 0.8596 ## Wald test: 1864 on 2 and 997 DF, p-value: &lt; 2.2e-16 On constate que les coefficients estimés “à la main” sont identiques aux coefficients issus de ivreg(). Les écart-types sont par contre différents, tout comme les statistiques de tests, les p-values etc. L’inférence correcte doit se faire à partir des calculs fournis par ivreg() "],["panel.html", "8 Donées de panel 8.1 Introduction 8.2 Le modèle à effets fixes 8.3 Le modèle à effets aléatoires 8.4 Le test d’Hausman 8.5 Implémentation sous R", " 8 Donées de panel 8.1 Introduction Dans les chapitres précédents, les données étaient implicitement des données en coupe, c’est-à-dire que chaque observation correspondant à un individu statistiques i différent. Les modèles étaient du type \\(y_i=\\beta_0+\\beta_1 x_{1i}+\\dots+\\beta_K x_{Ki}+\\epsilon_i\\). Il arrive frequemment que nous disposions de données de panel où l’on observe un groupe d’individus (indicées par \\(i=1\\dots n\\)) au travers de plusieurs périodes (indicées par \\(t=1\\dots T\\)). Le modèle s’écrit alors \\(y_it=\\beta_0+\\beta_1x_{1it}+\\dots+\\beta_Kx_{Kit}+\\alpha_i+\\epsilon_{it}\\) où \\(\\alpha_i\\) est un effet individuel propre à l’individu \\(i\\), et supposé constant au travers des \\(T\\) périodes d’observations, et où \\(\\epsilon_{it}\\) Le terme d’erreur \\(\\epsilon_{it}\\) sera considéré comme un terme d’erreur statndard des MCO : il respecte l’hypothèse d’exogénéité stricte ; il est également non autocorrélé, et homoscédastique. La différence entre les divers estimateurs utilisables sur données de panel reposent sur les hypothèses que l’on fera sur l’effet individuel \\(\\alpha_i\\). Si on considère que l’effet individuel \\(\\alpha_i\\) est non corrélé aux variables explicatives \\(x_1\\dots x_K\\), alors on va implémenter un modèle à effets aléatoires Si par contre on pense que l’effet individuel \\(\\alpha_i\\) est potentiellement corrélé à au moins une variable explicative \\(x_k\\), alors on va implémenter un modèle à effets fixes Le Test d’Hausman permet de trancher entre les deux modèles 8.2 Le modèle à effets fixes Dans le modèle à effets fixes, on considère que l’effet individuel \\(\\alpha_i\\) est potentiellement corrélé aux variables explicatives du modèle. Comme \\(\\alpha_i\\) est inobservé, on fait donc face à un bias de variable omises si on estime le modèles par MCO (ou MCG, voir la section sur le modèle à effets aléatoires). Comme \\(\\alpha_i\\) est supposé être constant au travers des \\(T\\) observations de l’individu \\(i\\), une solution serait d’inclure dans les variables explicatives \\(n\\) variables indicatrices \\(I_i\\), chacune valant 1 pour les \\(T\\) observations de l’individu \\(i\\), et 0 sinon. C’est l’estimateur dit “LSDV” (Least Squares Dummy Variables). En principe celà résoud le biais de variable omise, et donc permet d’estimer sans biais les coefficients des variables explicatuves d’intérêt. En pratique, néanmoins, l’estimateur LSDV imposerait d’inclure un très grand nombre de variables explicatives dans le modèle (une variable par individu), ce qui le rend difficilement implémentable. Une solution équivalente à l’estimateur LSDV est l’estimateur dit “within”. Cet estimateur utilise un résultat dit “Théorème de Frish-Waugh-Lowell” (cf cours de L3). Ce théorème nous permet d’estimer le modèle à effets fixes sans avoir à introduire les variables indicatrices. L’estimateur within consiste à appliquer les MCO sur données transformées où chaque variable est prise en différence par rapport à sa moyenne individuelle. En posant \\(\\tilde y_{it}=y_{it}-\\bar y_i\\) et \\(\\tilde x_{kit}=x_{kit}-\\bar x_{ki}\\), l’estimateur within applique les MCO au modèle \\(\\tilde y_{it} = \\beta_O + \\beta_1 \\tilde x_{1it}+\\dots+\\beta_K\\tilde x_{Kit}+\\epsilon_{it}\\). Le modèle à effet fixe permet l’estimation des \\(\\hat\\beta\\) en présence d’un effet individuel corrélé aux variables explicatives. Son principal défaut est qu’il interdit l’estimation des paramètres associés aux variables explicatives fices dans le temps (car dans ce cas \\(\\tilde x_{kit}=0\\)) 8.3 Le modèle à effets aléatoires Dans le modèle à effets aléatoires, on suppose que l’effet individuel \\(\\alpha_i\\) est non corrélé avec les variables explicatives du modèle. C’est une hypothèse forte qui, si elle n’est pas exacte, fera que nos \\(\\hat\\beta\\) seront biaisés. Si cette hypothèse est exacte, il sera possible d’estimer sans biais les \\(\\hat\\beta\\), y compris pour les variables fixes dans le temps (au contraire de l’estimateur witihn). On peut alors ré-écrire le modèle comme \\(y_it=\\beta_0+\\beta_1x_{1it}+\\dots+\\beta_Kx_{Kit}+\\alpha+u_{i}+\\epsilon_{it}\\) où \\(u_i\\) est un terme d’erreur individuel non corrélé aux variables explicatives, et également non corrélé à \\(\\epsilon_it\\). Le modèle à effets aléatoire utilise l’estimateur des Moindres Carrés Quasi Généralisés, car la présence du terme \\(u_i\\) induit une forme d’autocorrélation entre les observations d’un même individu. 8.4 Le test d’Hausman Afin de déterminer si l’hypothèse sous-jacente du modèle à effets aléatoires (la non corrélation entre l’effet individuel et les variables explicative) est valide, on peut implémenter le test d’Hausman. Ce test se base sur l’idée que si l’hypothèse de non corrélation entre \\(\\alpha_i\\) et les \\(x_k\\) est correcte, alos l’estimateur à effet fixe et l’estimateur à effet aléatoire devraient fournir des \\(\\hat\\beta\\) semblables (is sont tous les deux sans biais sous \\(H0\\), mais l’estimateur à effet aléatoire sera efficace). Par contre, si \\(H0\\) est fausse, alors ils devraient donner des \\(\\hat\\beta\\) différents car l’estimateur à effet aléatoire serait alors biaisé ; tandis que l’estimateur à effet fixe reste sans biais. La statistique du test d’Hausman se base donc sur une mesure de la différence entre les deux vecteurs de paramètres estimés. 8.5 Implémentation sous R Il existe plusieurs paquets permettant l’estimation sur données de panel sous R. On utilisera ici le paquet plm (Panel Linear Models) qui se spécialise, comme son nom l’indique, sur l’estimation des modèles linéaires sur données de Panel. Commençons par importer des données library(readr) # Pour importer des données panel &lt;- read_csv(&quot;https://raw.githubusercontent.com/ATerracol/P8Econ/master/data/panel101.csv&quot;) La base de données contient 400 observations, correspondant à 20 pays (variable country) observés sur 20 années (variable year) par pays. On y observe également les variables x1, x2 variables au cours du temps pour un pays donné ; et la variable x3 fixe au cours du temps pour un pays donné. La base contient également la variable y, qu’on va utiliser comme variable à expliquer, les x constituant les variables explicatives. Afin d’indiquer à plm quelles sont les variables permettant d’identifier les individus et les périodes, on va utilier la commande pdata.frame : library(plm) # On commence par charger le paquet plm # On crée un nouveau data.frame à l&#39;aide de la commande pdata.frame, en indiquant les variables indiquant l&#39;identifiant d&#39;invididu et de période panelb &lt;- pdata.frame(panel, index=c(&quot;country&quot;,&quot;year&quot;)) On peut ensuite estimer les modèles à effet fixess ou aléatoire en spécifiant l’option model= au sein de la command plm fixed &lt;-plm(data=panelb, y ~ x1 + x2 + x3, model=&quot;within&quot;) random &lt;-plm(data=panelb, y ~ x1 + x2 + x3, model=&quot;random&quot;) # On les compare summary(fixed) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = y ~ x1 + x2 + x3, data = panelb, model = &quot;within&quot;) ## ## Balanced Panel: n = 20, T = 20, N = 400 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -3.276772 -0.643815 0.029442 0.628518 3.002143 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## x1 0.991473 0.102664 9.6575 &lt; 2.2e-16 *** ## x2 0.970439 0.053748 18.0552 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 826.76 ## Residual Sum of Squares: 385.74 ## R-Squared: 0.53343 ## Adj. R-Squared: 0.50751 ## F-statistic: 216.083 on 2 and 378 DF, p-value: &lt; 2.22e-16 summary(random) ## Oneway (individual) effect Random Effect Model ## (Swamy-Arora&#39;s transformation) ## ## Call: ## plm(formula = y ~ x1 + x2 + x3, data = panelb, model = &quot;random&quot;) ## ## Balanced Panel: n = 20, T = 20, N = 400 ## ## Effects: ## var std.dev share ## idiosyncratic 1.020 1.010 0.325 ## individual 2.122 1.457 0.675 ## theta: 0.8468 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -3.042938 -0.701596 0.033314 0.649309 3.512979 ## ## Coefficients: ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept) 0.744259 0.339807 2.1902 0.02851 * ## x1 0.861133 0.098770 8.7186 &lt; 2e-16 *** ## x2 0.977088 0.054659 17.8761 &lt; 2e-16 *** ## x3 3.429725 1.720292 1.9937 0.04619 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 851.65 ## Residual Sum of Squares: 418.66 ## R-Squared: 0.50841 ## Adj. R-Squared: 0.50468 ## Chisq: 409.544 on 3 DF, p-value: &lt; 2.22e-16 On constate que le modèle à effets fixes ne fournit pas de valeur \\(\\hat\\beta\\) pour la variable \\(x_3\\) (qui est fixe dans le temps), ni pour la constante (pour des raisons de normalisations, afin d’éviter la colinéarité parfaite avec les effets individuels). En revanche, comme expliqué dans les sections ci-dessus, le modèle à effets aléatoire permet d’estimer les paramètres de toutes les variables, y compris de celles qui sont fixes dans le temps. Si le modèle à effets aléatoires semble attrayant car il permet d’estimer les paramètres de toutes les variables, il reste toutefois dépendant de l’hypothèse d’indépendance entre les effets individuels \\(\\alpha_i\\) et les variables explicatives du modèle. Si cette hypothèse n’est pas vérifiée, alors les paramètres du modèle à effet aléatoire seront biaisés. Afin de tester cette hypothèse de non corrélation, et trancher entre un modèle à effets aléatoire et un modèle à effets fixes, on peut utiliser un test d’Hausmann, à l’aide de la commande phtest incluse dans le paquet plm : phtest(fixed,random) ## ## Hausman Test ## ## data: y ~ x1 + x2 + x3 ## chisq = 21.648, df = 2, p-value = 1.992e-05 ## alternative hypothesis: one model is inconsistent La p-value nous indique que l’on rejette l’hypothèse nulle de non corréllation entre les effets individuels et les variables explicatives au seuil de 1 %. On en conclut donc que le modèle à effets aléatoire est biaisé et inadéquat dans notre situation, et on choisit de se baser sur les résultats du modèle à effets fixes. "]]
